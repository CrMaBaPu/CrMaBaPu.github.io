{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CEBRA neural latent embeddings","text":"<p><code>[Last update: September 16, 2024]</code></p> <pre><code>Period:     2025-06 - 2025-08\nStatus:     in progress\n\nAuthor(s):  Cristina Maria Bayer\nContact:    bayer-cristina@t-online.de\n</code></pre>"},{"location":"#links","title":"Links","text":""},{"location":"#github-repository","title":"GitHub Repository","text":"<p>The GitHub Repository with all the code can be assessed here: github.com</p>"},{"location":"#documentation","title":"Documentation","text":"<p>The Documentation can be assessed here: mkdocs</p>"},{"location":"#project-description","title":"Project description","text":"<p>This project focuses on applying CEBRA (Contrastive Embedding for Behavior and Representation Analysis) to EEG data collected during a 20-minute immersive VR experience from the AVR Project. The dataset includes neural responses to a sequence of different videos shown in VR. The primary goal is to implement and explore the application of CEBRA for generating neural embeddings from EEG signals to better understand patterns and representations associated with different video stimuli. Preprocessed EEG data was provided by the AVR Project, allowing for direct experimentation with the CEBRA framework.</p>"},{"location":"#project-specific-challenges","title":"Project-Specific Challenges","text":"<p>Complex Stimulus Structure:</p> <ul> <li>Habituation Effects: Neural responses to 'b' may change across repetitions</li> <li>Context Effects: Same 'b' video may evoke different responses depending on preceding content</li> <li>Memory Interference: Later 'b' presentations may activate memory traces from earlier ones</li> <li>Expectation Effects: Participants may learn the pattern and anticipate repeated videos</li> </ul> <p>Continuous Labeling Interference:</p> <ul> <li>Cognitive Load: Dual-task demands affect natural emotional responses</li> <li>Motor Artifacts: Hand movements for labeling create systematic EEG noise</li> <li>Attention Division: Split attention between content and introspective monitoring</li> <li>Response Bias: The labeling process may alter the emotional experience itself</li> </ul> <p>Temporal Alignment Complexity:</p> <ul> <li>Multiple overlapping temporal structures (video timing, emotional dynamics, labeling timing, neural timing)</li> <li>Different frequency bands have different temporal characteristics</li> <li>Emotional responses may lag or persist beyond stimulus boundaries</li> <li>Individual differences in emotional response timing</li> </ul>"},{"location":"#experimental-design-strengths","title":"Experimental Design Strengths","text":"<p>Built-in Validation Structure:</p> <ul> <li>Repeated 'b' videos provide natural ground truth for embedding consistency</li> <li>Enable assessment of habituation vs. consistency in neural responses</li> <li>Allow detection of context effects (same stimulus, different preceding context)</li> <li>Serve as quality control for artifact removal effectiveness</li> </ul> <p>Rich Temporal Dynamics:</p> <ul> <li>Continuous emotional experience rather than discrete trials</li> <li>Natural emotional flow and transition patterns</li> <li>Captures emotional persistence and carryover effects</li> <li>Realistic emotional dynamics with overlapping responses</li> </ul> <p>Multi-dimensional Emotional Space:</p> <ul> <li>Different videos likely span various emotional dimensions</li> <li>Enables exploration of emotional transition dynamics</li> <li>Captures individual differences in emotional response patterns</li> <li>Provides diverse content for manifold discovery</li> </ul>"},{"location":"components/dataset-collection/","title":"CEBRA Multi-Session Architecture Modifications","text":""},{"location":"components/dataset-collection/#summary","title":"Summary","text":"<p>This document outlines critical modifications made to the CEBRA (Consistent EmBeddings of high-dimensional Recordings using Auxiliary variables) codebase to resolve training failures and implement a single shared model architecture for multi-session datasets. The changes address two primary issues: missing required arguments in dataset dimension handling and iterator compatibility problems in the solver model.</p>"},{"location":"components/dataset-collection/#issues-addressed","title":"Issues Addressed","text":""},{"location":"components/dataset-collection/#issue-1-datasetcollection-input-dimension-error","title":"Issue 1: DatasetCollection Input Dimension Error","text":"<p>Error: <code>Training failed: DatasetCollection.get_input_dimension() missing 1 required positional argument: 'session_id'</code></p>"},{"location":"components/dataset-collection/#issue-2-model-iterator-compatibility-error","title":"Issue 2: Model Iterator Compatibility Error","text":"<p>Error: <code>Training failed: 'Offset10Model' object is not iterable</code></p>"},{"location":"components/dataset-collection/#modifications-overview","title":"Modifications Overview","text":""},{"location":"components/dataset-collection/#1-datasetcollection-class-cebra_datamultisessiondataset","title":"1. DatasetCollection Class (<code>cebra_data.MultiSessionDataset</code>)","text":""},{"location":"components/dataset-collection/#problem","title":"Problem","text":"<p>The original implementation required a <code>session_id</code> parameter for <code>get_input_dimension()</code> but the property <code>input_dimension</code> didn't provide one, causing training failures.</p>"},{"location":"components/dataset-collection/#solution","title":"Solution","text":"<p>Modified the class to provide both a property fallback and an optional parameter method:</p> <pre><code>@property\ndef input_dimension(self):\n    # Return the input dimension of the first session as fallback\n    return self._datasets[0].input_dimension\n\ndef get_input_dimension(self, session_id: Optional[int] = None) -&gt; int:\n    \"\"\"Get the feature dimension of the required session.\n\n    Args:\n        session_id: The session ID, an integer between 0 and\n            :py:attr:`num_sessions`. If None, returns first session dimension.\n\n    Returns:\n        A single session input dimension for the requested session id.\n    \"\"\"\n    if session_id is None:\n        # fallback to first session input dimension\n        return self._datasets[0].input_dimension\n    return self.get_session(session_id).input_dimension\n</code></pre>"},{"location":"components/dataset-collection/#changes-made","title":"Changes Made","text":"<ul> <li>Added: Property <code>input_dimension</code> that returns the first session's input dimension as a fallback</li> <li>Modified: <code>get_input_dimension()</code> method to accept optional <code>session_id</code> parameter</li> <li>Behavior: When <code>session_id</code> is <code>None</code>, returns the first session's input dimension; otherwise, returns the specified session's dimension</li> </ul>"},{"location":"components/dataset-collection/#2-multisessionloader-class","title":"2. MultiSessionLoader Class","text":""},{"location":"components/dataset-collection/#problem_1","title":"Problem","text":"<p>The solver expected the loader to have an <code>input_dimension</code> property, but <code>MultiSessionLoader</code> didn't implement this property.</p>"},{"location":"components/dataset-collection/#solution_1","title":"Solution","text":"<p>Added the missing property to delegate to the dataset:</p> <pre><code>@property\ndef input_dimension(self):\n    return self.dataset.input_dimension\n</code></pre>"},{"location":"components/dataset-collection/#changes-made_1","title":"Changes Made","text":"<ul> <li>Added: <code>input_dimension</code> property that delegates to <code>self.dataset.input_dimension</code></li> <li>Purpose: Provides consistent interface for dimension access across different loader types</li> </ul>"},{"location":"components/dataset-collection/#3-solver-architecture-single-model-implementation","title":"3. Solver Architecture: Single Model Implementation","text":""},{"location":"components/dataset-collection/#problem_2","title":"Problem","text":"<p>The original implementation treated <code>self.model</code> as an iterable collection of models (one per session), but the goal was to use a single shared model across all sessions.</p>"},{"location":"components/dataset-collection/#solution_2","title":"Solution","text":"<p>Refactored multiple methods to treat <code>self.model</code> as a single model instance rather than a collection:</p>"},{"location":"components/dataset-collection/#31-parameters-method","title":"3.1 Parameters Method","text":"<pre><code>def parameters(self, session_id: Optional[int] = None):\n    \"\"\"Iterate over all parameters.\n\n    Args:\n        session_id: The session ID, an :py:class:`int` between 0 and\n            the number of sessions -1 for multisession, and set to\n            ``None`` for single session.\n\n    Yields:\n        The parameters of the model.\n    \"\"\"\n    self._check_is_session_id_valid(session_id=session_id)\n\n    for parameter in self.model.parameters():\n        yield parameter\n\n    for parameter in self.criterion.parameters():\n        yield parameter\n</code></pre> <p>Change: Removed indexing <code>self.model[session_id]</code> and directly iterate over <code>self.model.parameters()</code></p>"},{"location":"components/dataset-collection/#32-inference-method","title":"3.2 Inference Method","text":"<pre><code>def _inference(self, batches: List[cebra.data.Batch]) -&gt; cebra.data.Batch:\n    \"\"\"Given batches of input examples, computes the feature representations/embeddings.\"\"\"\n    refs = []\n    poss = []\n    negs = []\n\n    for batch in batches:\n        batch.to(self.device)\n        refs.append(self.model(batch.reference))\n        poss.append(self.model(batch.positive))\n        negs.append(self.model(batch.negative))\n\n    ref = torch.stack(refs, dim=0)\n    pos = torch.stack(poss, dim=0)\n    neg = torch.stack(negs, dim=0)\n\n    pos = self._mix(pos, batches[0].index_reversed)\n    num_features = neg.shape[2]\n\n    return cebra.data.Batch(\n        reference=ref.view(-1, num_features),\n        positive=pos.view(-1, num_features),\n        negative=neg.view(-1, num_features),\n    )\n</code></pre> <p>Change: Removed <code>zip(batches, self.model)</code> pattern and use single <code>self.model</code> for all batches</p>"},{"location":"components/dataset-collection/#33-get-model-method","title":"3.3 Get Model Method","text":"<pre><code>def _get_model(self, session_id: Optional[int] = None):\n    \"\"\"Get the model for the given session ID.\"\"\"\n    self._check_is_session_id_valid(session_id=session_id)\n    self._check_is_fitted()\n    return self.model\n</code></pre> <p>Change: Return <code>self.model</code> directly instead of <code>self.model[session_id]</code></p>"},{"location":"components/dataset-collection/#34-validation-method","title":"3.4 Validation Method","text":"<pre><code>def validation(self, loader, session_id: Optional[int] = None):\n    \"\"\"Compute score of the model on data.\"\"\"\n    assert session_id is not None\n\n    iterator = self._get_loader(loader)\n    total_loss = Meter()\n    self.model.eval()\n    for _, batch in iterator:\n        prediction = self._single_model_inference(batch, self.model)\n        loss, _, _ = self.criterion(prediction.reference,\n                                    prediction.positive,\n                                    prediction.negative)\n        total_loss.add(loss.item())\n    return total_loss.average\n</code></pre> <p>Change: Use <code>self.model.eval()</code> and <code>self.model</code> instead of <code>self.model[session_id]</code></p>"},{"location":"components/dataset-collection/#implementation-guide","title":"Implementation Guide","text":"<p>To implement these changes in your CEBRA clone:</p>"},{"location":"components/dataset-collection/#step-1-locate-target-files","title":"Step 1: Locate Target Files","text":"<ul> <li>Find the <code>DatasetCollection</code> class in the multi-session dataset module</li> <li>Locate the <code>MultiSessionLoader</code> class in the data loading module</li> <li>Find the solver class containing the methods listed above</li> </ul>"},{"location":"components/dataset-collection/#step-2-apply-datasetcollection-changes","title":"Step 2: Apply DatasetCollection Changes","text":"<ol> <li>Replace the <code>input_dimension</code> property implementation</li> <li>Modify the <code>get_input_dimension</code> method signature and implementation</li> <li>Ensure proper import of <code>Optional</code> from <code>typing</code></li> </ol>"},{"location":"components/dataset-collection/#step-3-apply-multisessionloader-changes","title":"Step 3: Apply MultiSessionLoader Changes","text":"<ol> <li>Add the <code>input_dimension</code> property to the class</li> </ol>"},{"location":"components/dataset-collection/#step-4-apply-solver-changes","title":"Step 4: Apply Solver Changes","text":"<ol> <li>Update all four methods (<code>parameters</code>, <code>_inference</code>, <code>_get_model</code>, <code>validation</code>)</li> <li>Replace model indexing patterns with direct model usage</li> <li>Ensure consistent single-model treatment across all methods</li> </ol>"},{"location":"components/dataset-collection/#step-5-testing","title":"Step 5: Testing","text":"<ol> <li>Run multi-session training to verify the fixes resolve both error conditions</li> </ol>"},{"location":"components/dataset-collection/#technical-impact","title":"Technical Impact","text":""},{"location":"components/dataset-collection/#architectural-changes","title":"Architectural Changes","text":"<ul> <li>Model Sharing: Transition from per-session models to single shared model architecture</li> <li>Interface Consistency: Standardized dimension access patterns across loaders and datasets</li> </ul>"},{"location":"components/dataset-collection/#backward-compatibility","title":"Backward Compatibility","text":"<p>These changes maintain backward compatibility by:</p> <ul> <li>Preserving existing method signatures</li> <li>Providing fallback behaviors for optional parameters</li> <li>Maintaining expected return types and interfaces</li> </ul>"},{"location":"components/tensor-dataset/","title":"TensorDataset Design","text":""},{"location":"components/tensor-dataset/#conceptual-role","title":"Conceptual Role","text":"<p>The TensorDataset serves as the fundamental building block for organizing and accessing individual EEG recording sessions in your multi-session analysis. It acts as a bridge between raw neural data and the sophisticated machine learning framework of CEBRA, providing a structured way to handle the temporal and spatial complexity of EEG recordings.</p>"},{"location":"components/tensor-dataset/#core-design-philosophy","title":"Core Design Philosophy","text":""},{"location":"components/tensor-dataset/#1-unified-data-representation","title":"1. Unified Data Representation","text":"<p>TensorDataset creates a unified interface for neural data by:</p> <ul> <li>Standardizing formats: Converting various data formats (NumPy arrays, PyTorch tensors) into a consistent representation</li> <li>Handling heterogeneity: Accommodating different data types (continuous behavioral variables, discrete task labels)</li> <li>Maintaining relationships: Preserving the temporal alignment between neural activity and auxiliary variables</li> </ul>"},{"location":"components/tensor-dataset/#2-auxiliary-variable-integration","title":"2. Auxiliary Variable Integration","text":"<p>The dataset design explicitly supports auxiliary variables that guide the learning process:</p> <pre><code># Conceptual structure\nTensorDataset(\n    neural=eeg_data,        # Shape: (time_points, channels)\n    continuous=behavior,    # Shape: (time_points, behavioral_dims)\n    discrete=task_labels   # Shape: (time_points,)\n)\n</code></pre> <p>This tri-partite structure ensures that: - Neural data provides the high-dimensional observations - Continuous variables capture smooth behavioral changes (e.g., attention levels, arousal) - Discrete variables encode categorical information (e.g., task conditions, stimulus types)</p>"},{"location":"components/tensor-dataset/#data-structure-and-organization","title":"Data Structure and Organization","text":""},{"location":"components/tensor-dataset/#temporal-alignment","title":"Temporal Alignment","text":"<p>The dataset maintains strict temporal alignment across all data modalities:</p> \\[\\mathbf{X}(t) = \\begin{bmatrix} \\mathbf{x}_{neural}(t) \\\\ \\mathbf{x}_{continuous}(t) \\\\ \\mathbf{x}_{discrete}(t) \\end{bmatrix}\\] <p>Where: - \\(\\mathbf{x}_{neural}(t) \\in \\mathbb{R}^{N_{channels}}\\) is the EEG activity at time \\(t\\) - \\(\\mathbf{x}_{continuous}(t) \\in \\mathbb{R}^{N_{behavioral}}\\) are continuous behavioral variables - \\(\\mathbf{x}_{discrete}(t) \\in \\mathbb{Z}^{N_{categorical}}\\) are discrete task variables</p>"},{"location":"components/tensor-dataset/#offset-mechanism","title":"Offset Mechanism","text":"<p>The offset property enables temporal context extraction:</p> <pre><code>offset = Offset(left=5, right=5)  # 10 time points total\n</code></pre> <p>This creates temporal windows around each reference time point:</p> \\[\\mathbf{X}_{window}(t) = [\\mathbf{X}(t-5), \\mathbf{X}(t-4), \\ldots, \\mathbf{X}(t+4), \\mathbf{X}(t+5)]\\] <p>The offset mechanism serves multiple purposes:</p> <ol> <li>Temporal context: Captures neural dynamics leading up to and following events</li> <li>Noise reduction: Smooths out transient fluctuations</li> <li>Feature enrichment: Provides more information for pattern recognition</li> <li>Causal modeling: Enables analysis of cause-effect relationships</li> </ol>"},{"location":"components/tensor-dataset/#functional-capabilities","title":"Functional Capabilities","text":""},{"location":"components/tensor-dataset/#1-dynamic-indexing","title":"1. Dynamic Indexing","text":"<p>The <code>expand_index</code> method transforms simple time indices into rich temporal windows:</p> <pre><code># Simple index\nindex = [100, 150, 200]\n\n# Expanded with offset\nexpanded = dataset.expand_index(index)\n# Shape: (3, 11) for offset=5 left and right\n</code></pre> <p>This expansion enables: - Multi-scale analysis: Examining neural activity at different temporal scales - Context preservation: Maintaining information about surrounding time points - Flexible sampling: Adapting to different analysis requirements</p>"},{"location":"components/tensor-dataset/#2-data-type-validation","title":"2. Data Type Validation","text":"<p>The dataset enforces data type consistency:</p> <ul> <li>Neural data: Must be floating-point (typically float32 or float64)</li> <li>Continuous variables: Must be floating-point for smooth interpolation</li> <li>Discrete variables: Must be integer types for categorical encoding</li> </ul> <p>This validation prevents common errors and ensures compatibility with downstream analysis tools.</p>"},{"location":"components/tensor-dataset/#3-device-management","title":"3. Device Management","text":"<p>The dataset handles device placement for efficient computation:</p> <pre><code>dataset = TensorDataset(neural_data, device=\"cuda\")\n</code></pre> <p>This enables: - GPU acceleration: Faster computation for large datasets - Memory management: Efficient use of device memory - Seamless integration: Compatibility with PyTorch-based models</p>"},{"location":"components/tensor-dataset/#data-access-patterns","title":"Data Access Patterns","text":""},{"location":"components/tensor-dataset/#temporal-window-retrieval","title":"Temporal Window Retrieval","text":"<p>When accessing data, the dataset returns temporal windows rather than single time points:</p> <pre><code># Access single time point\ndata = dataset[t]\n# Returns shape: (channels, temporal_window)\n</code></pre> <p>This access pattern: - Preserves temporal structure: Maintains the sequential nature of neural data - Enables convolution: Compatible with convolutional neural networks - Supports temporal modeling: Facilitates analysis of temporal dependencies</p>"},{"location":"components/tensor-dataset/#batch-processing","title":"Batch Processing","text":"<p>The dataset supports efficient batch processing:</p> <pre><code># Batch access\nbatch_data = dataset[batch_indices]\n# Returns shape: (batch_size, channels, temporal_window)\n</code></pre> <p>Batch processing enables: - Parallel computation: Simultaneous processing of multiple samples - Memory efficiency: Optimal use of computational resources - Scalability: Handling large datasets efficiently</p>"},{"location":"components/tensor-dataset/#integration-with-cebra-framework","title":"Integration with CEBRA Framework","text":""},{"location":"components/tensor-dataset/#auxiliary-variable-utilization","title":"Auxiliary Variable Utilization","text":"<p>The dataset's auxiliary variables directly inform CEBRA's contrastive learning:</p> <ol> <li>Positive pairs: Samples with similar auxiliary variables</li> <li>Negative pairs: Samples with different auxiliary variables</li> <li>Reference samples: Anchor points for comparison</li> </ol>"},{"location":"components/tensor-dataset/#temporal-consistency","title":"Temporal Consistency","text":"<p>The offset mechanism ensures temporal consistency in the learned representations:</p> <ul> <li>Smooth transitions: Nearby time points have similar representations</li> <li>Temporal invariance: Representations are stable across small time shifts</li> <li>Causal relationships: Past context informs current representations</li> </ul>"},{"location":"components/tensor-dataset/#advantages-for-eeg-analysis","title":"Advantages for EEG Analysis","text":""},{"location":"components/tensor-dataset/#1-temporal-richness","title":"1. Temporal Richness","text":"<p>EEG data has complex temporal structure that the TensorDataset captures:</p> <ul> <li>Fast dynamics: Capturing millisecond-scale neural events</li> <li>Slow trends: Preserving longer-term changes in brain state</li> <li>Rhythmic patterns: Maintaining oscillatory components</li> </ul>"},{"location":"components/tensor-dataset/#2-multimodal-integration","title":"2. Multimodal Integration","text":"<p>The dataset naturally handles multiple data streams:</p> <ul> <li>Neural activity: High-dimensional EEG recordings</li> <li>Behavioral measures: Continuous performance metrics</li> <li>Task structure: Discrete experimental conditions</li> </ul>"},{"location":"components/tensor-dataset/#3-preprocessing-integration","title":"3. Preprocessing Integration","text":"<p>The dataset can incorporate preprocessing steps:</p> <ul> <li>Artifact removal: Filtering out noise and artifacts</li> <li>Normalization: Standardizing signal amplitudes</li> <li>Feature extraction: Computing derived measures</li> </ul>"},{"location":"components/tensor-dataset/#memory-and-computational-considerations","title":"Memory and Computational Considerations","text":""},{"location":"components/tensor-dataset/#memory-optimization","title":"Memory Optimization","text":"<p>The dataset implements several memory optimization strategies:</p> <ol> <li>Lazy loading: Loading data only when accessed</li> <li>Efficient storage: Using appropriate data types</li> <li>Device management: Optimal placement of data in memory</li> </ol>"},{"location":"components/tensor-dataset/#computational-efficiency","title":"Computational Efficiency","text":"<p>The design prioritizes computational efficiency:</p> <ol> <li>Vectorized operations: Using efficient tensor operations</li> <li>Batch processing: Minimizing loop overhead</li> <li>GPU compatibility: Leveraging parallel processing</li> </ol>"},{"location":"components/tensor-dataset/#limitations-and-design-trade-offs","title":"Limitations and Design Trade-offs","text":""},{"location":"components/tensor-dataset/#memory-requirements","title":"Memory Requirements","text":"<p>The temporal window approach requires more memory:</p> \\[\\text{Memory} = N_{samples} \\times N_{channels} \\times N_{timepoints} \\times \\text{dtype\\_size}\\] <p>Where \\(N_{timepoints}\\) is determined by the offset size.</p>"},{"location":"components/tensor-dataset/#temporal-resolution","title":"Temporal Resolution","text":"<p>The offset mechanism imposes constraints on temporal resolution:</p> <ul> <li>Fixed windows: All samples use the same temporal context</li> <li>Boundary effects: Samples near recording boundaries may be excluded</li> <li>Resolution trade-offs: Larger offsets capture more context but reduce temporal precision</li> </ul>"},{"location":"components/tensor-dataset/#future-enhancements","title":"Future Enhancements","text":""},{"location":"components/tensor-dataset/#adaptive-offsets","title":"Adaptive Offsets","text":"<p>Future versions might support adaptive offset sizes:</p> <ul> <li>Task-dependent: Different offsets for different task conditions</li> <li>Data-driven: Automatically determining optimal offset sizes</li> <li>Hierarchical: Multi-scale temporal analysis</li> </ul>"},{"location":"components/tensor-dataset/#advanced-preprocessing","title":"Advanced Preprocessing","text":"<p>Enhanced preprocessing capabilities could include:</p> <ul> <li>Real-time processing: Online artifact removal and normalization</li> <li>Adaptive filtering: Context-dependent signal processing</li> <li>Quality assessment: Automatic data quality evaluation</li> </ul> <p>The TensorDataset design provides a robust foundation for handling the complexity of EEG data while maintaining the flexibility needed for sophisticated neural state analysis. Its integration with auxiliary variables and temporal windowing makes it particularly well-suited for CEBRA's contrastive learning approach, enabling the discovery of meaningful patterns in neural activity across different subjects and experimental conditions.</p>"},{"location":"dataset/data/","title":"Data Overview","text":"<p>[Note!] This chapter was mainly formulated on information provided by the AVR Porject documentation. </p>"},{"location":"dataset/data/#eeg-data","title":"EEG Data","text":"<p>The dataset consists of 54 subjects each with a measurement of 23 minutes with a 64 channel EEG. </p>"},{"location":"dataset/data/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<p>The EEG data processing follows a comprehensive pipeline that transforms raw LSL-output format (<code>.xdf</code>) files into analysis-ready datasets using BIDS-compatible structure. </p>"},{"location":"dataset/data/#eeg-signal-processing","title":"EEG Signal Processing","text":"<p>The EEG preprocessing pipeline implements a robust multi-stage approach to ensure high-quality neural signal extraction:</p> <p>Initial Processing: Raw EEG data undergoes cropping of 2.5 seconds from both start and end to eliminate edge artifacts that commonly occur during recording initiation and termination.</p> <p>Signal Cleaning: The cleaning process involves several critical steps. Power line interference is removed at 50 Hz, followed by re-referencing to the average across all channels to reduce common noise. Band-pass filtering between 0.1 and 45 Hz isolates the neurophysiologically relevant frequency range while removing low-frequency drift and high-frequency noise.</p> <p>Epoch Processing: The continuous EEG signal is segmented into 10-second epochs for systematic analysis. Bad channels and epochs are identified and either rejected or interpolated using the <code>autoreject</code> algorithm applied to 1 Hz-filtered data, ensuring optimal data quality while preserving maximum usable data.</p> <p>Artifact Removal: Independent Component Analysis (ICA) is employed to identify and remove physiological artifacts including eye movements, cardiac interference, and muscle activity that can contaminate neural signals.</p> <p>Quality Control: A final threshold check excludes participants with more than 30% noisy epochs remaining after preprocessing, ensuring only high-quality datasets proceed to analysis.</p>"},{"location":"dataset/data/#behavioural-data","title":"Behavioural Data","text":"<p>Using a internally developed davice the participants continuously labeled their emotional states. </p> <p>The input from these devices was transformed to a valence arrousal scale that is synchronized to the eeg data. </p>"},{"location":"dataset/data/#challenges","title":"Challenges","text":"<ul> <li>Incomplete Emotional Coverage: For example, in the case of sub-001 and sub-002, visualizations show that participants did not experience (or at least did not label) the full emotional spectrum in equal amounts. </li> <li>Data Imbalance and Inter-Subject Variability: There is a clear imbalance in the data, with certain emotion zones underrepresented. Additionally, strong personal differences in how emotions were experienced and reported create further complexity.</li> <li>Potential Mislabeling: It is possible that participants did experience the full range of emotions but failed to label them correctly. This presents a validation challenge, as we have no ground truth to verify the accuracy of self-reported emotional states.</li> </ul> <p>e.g sub-001 valence  e.g sub-002 valence  </p>"},{"location":"dataset/data/#vr-specific-challenges","title":"VR-Specific Challenges","text":"<p>Participant Experience Effects: - VR sickness can confound emotional responses - Novelty effects may overshadow target emotional content - Individual differences in VR adaptation and comfort - Dual-task interference from continuous emotional labeling</p>"},{"location":"dataset/multi-subject-design/","title":"Study Design","text":"<p>[Note !] This chapter was  mainly formulated on information provided by the AVR Porject documentation. </p> <p>The resulting dataset captures multimodal physiological and behavioral data in response to emotionally evocative VR stimuli.</p>"},{"location":"dataset/multi-subject-design/#overview","title":"Overview","text":"<p>The experimental pipeline consists of several sequential phases:</p> <ol> <li>Pre-Survey</li> <li>Setup</li> <li>Resting State (Pre)</li> <li>Training</li> <li>Experiment</li> <li>Resting State (Post)</li> <li>Post-Survey</li> </ol>"},{"location":"dataset/multi-subject-design/#1-pre-survey","title":"1. Pre-Survey","text":"<p>Participants begin by completing a pre-survey assessing their baseline mood, demographic information, and individual differences in emotional traits.</p>"},{"location":"dataset/multi-subject-design/#2-setup","title":"2. Setup","text":"<p>Participants are instrumented with a range of biosensors to monitor:</p> <ul> <li>EEG (electroencephalogram)</li> <li>ECG (electrocardiogram)</li> <li>PPG (photoplethysmogram)</li> <li>GSR (galvanic skin response)</li> <li>Respiration belt</li> <li>Eye tracking</li> </ul>"},{"location":"dataset/multi-subject-design/#3-resting-state-pre","title":"3. Resting State (Pre)","text":"<p>Participants undergo two resting-state conditions:</p> <ul> <li>Eyes Open</li> <li>Eyes Closed</li> </ul> <p>This baseline data provides a physiological benchmark for comparison with task-related responses.</p>"},{"location":"dataset/multi-subject-design/#4-training","title":"4. Training","text":"<p>Participants are introduced to the Flubber interface in VR, a dynamic rating tool used to continuously report their affective state (valence and arousal) during the experiment.</p> <ul> <li>Valence: Negative to Positive</li> <li>Arousal: Low to High</li> <li>The Flubber pulsates (slow/fast) and changes shape (regular/irregular) to reflect arousal and valence.</li> </ul>"},{"location":"dataset/multi-subject-design/#5-experiment","title":"5. Experiment","text":"<p>Participants experience a sequence of emotionally charged VR scenarios (e.g., spaceship, invasion, asteroids, underwood). During the entire session:</p> <ul> <li>They use a handheld VR controller to continuously rate their emotional state via the Flubber interface.</li> <li>At the end of the session, participants provide summary ratings for each scene.</li> </ul>"},{"location":"dataset/multi-subject-design/#6-resting-state-post","title":"6. Resting State (Post)","text":"<p>The resting state procedure is repeated (eyes open and closed) to observe any post-experiment physiological shifts.</p>"},{"location":"dataset/multi-subject-design/#7-post-survey","title":"7. Post-Survey","text":"<p>A final survey is completed, collecting subjective feedback and post-session emotional self-assessment.</p>"},{"location":"dataset/multi-subject-design/#data-description","title":"Data Description","text":"<p>The dataset derived from this experiment includes:</p> <ul> <li>Physiological Signals: EEG, ECG, PPG, GSR, respiration, and eye tracking</li> <li>VR Controller Data: Continuous Flubber ratings of valence and arousal</li> <li>Survey Data: Pre/post emotional states, participant demographics, and scene-specific ratings</li> </ul>"},{"location":"methods/Phase_1/","title":"Inital Exploration of EEG Emebddings","text":""},{"location":"methods/Phase_1/#overview","title":"Overview","text":"<p>The initial phase of this project focused on exploring neural embeddings learned from preprocessed EEG data using the CEBRA (Contrastive Embedding for Behavioral and neural Representation Analysis) framework. This step served as an unsupervised, exploratory analysis to evaluate the latent structure of neural activity and its variability depending on the selected EEG channels and subject identity.</p>"},{"location":"methods/Phase_1/#objectives","title":"Objectives","text":"<ul> <li>Test the CEBRA embedding framework on preprocessed EEG recordings.</li> <li>Investigate how the resulting embeddings vary across different channel subsets.</li> <li>Visualize the temporal dynamics of EEG representations in a reduced latent space.</li> <li>Establish a foundation for future extension and comparisons across subjects or conditions.</li> </ul>"},{"location":"methods/Phase_1/#exploration","title":"Exploration","text":"<ul> <li>Channel Effects: Preliminary visual differences were examined when using different sets of EEG channels (e.g., frontal vs. parietal).</li> <li>Subject-Specific Embedding: Analysis was limited to a single subject. Generalization across subjects will be addressed in future stages.</li> <li>Signal Length: Time series data length was considered for ensuring meaningful temporal embedding.</li> </ul>"},{"location":"methods/Phase_1/#data-preparation","title":"Data Preparation","text":""},{"location":"methods/Phase_1/#data-loading","title":"Data loading","text":"<p>A custom data loading pipeline was implemented to handle both preprocessed (.fif) and raw (.edf) EEG file formats. The <code>load_subject()</code> function serves as the core loading mechanism, accepting a subject directory path and data type specification. For this analysis, preprocessed data in MNE-Python FIF format was utilized, specifically targeting files with the naming pattern <code>*before_ica.fif</code>, indicating data that had undergone preprocessing but prior to independent component analysis.</p> <p>Preprocessed EEG data were loaded for a single subject (e.g <code>sub-020</code>) from a cleaned dataset directory. The data had already undergone necessary preprocessing (e.g., filtering, artifact removal, ICA), and therefore represent neural activity in a clean and interpretable form.The time series data were transposed and concatenated to form a matrix of shape <code>(n_timepoints, n_channels)</code>, suitable for input to the CEBRA model.</p> <pre><code>picks = mne.pick_types(raw.info, eeg=True, eog=False)\ndata = raw.get_data(picks=picks).T  # shape: (n_times, n_channels)\n</code></pre> <p>Configurations tested include:  * Time cropping to specific intervals or full session. * Frequency filtering to extract EEG bands (theta, alpha, beta, gamma). * Cannel subset selection  (frontal, central-parietal, combined, or all channels).</p>"},{"location":"methods/Phase_1/#embedding-via-cebra","title":"Embedding via CEBRA","text":"<p>The CEBRA model was initialized with the following configuration:</p> <ul> <li>Model architecture: Offset-based temporal contrastive model (<code>offset10-model</code>)</li> <li>Training mode: Unsupervised (<code>conditional=None</code>)</li> <li>Distance metric: Cosine similarity</li> <li>Latent dimensionality: 3 (for visualization)</li> <li>Hardware: GPU used if available</li> </ul> <pre><code>cebra_model = CEBRA(\n    model_architecture=\"offset10-model\",\n    batch_size=512,\n    learning_rate=3e-4,\n    temperature_mode='constant',\n    temperature=1.12,\n    max_iterations=5000,\n    conditional=None,\n    output_dimension=3,\n    distance='cosine',\n    device=\"cuda_if_available\",\n)\n</code></pre> <p>After training, the latent representation of the EEG time series was computed:</p> <pre><code>embedding = cebra_model.transform(X)\n</code></pre>"},{"location":"methods/Phase_1/#visualization","title":"Visualization","text":"<p>The embeddings were visualized using an interactive 3D scatter plot, with time used as the color gradient to illustrate temporal progression of brain states.</p> <p>Due to the large number of timepoints, a downsampling step was applied before visualization to improve rendering performance and clarity</p> <p>The resulting plot displays the trajectory of brain states over time in the learned latent space:</p> <p><pre><code>fig = plot_embedding_interactive(\n    embedding_small,\n    embedding_labels=times_small,\n    title=\"CEBRA Embedding (Brain States)\",\n    markersize=5,\n    cmap=\"rainbow\"\n)\n</code></pre> A custom valence-arousal color encoding system has been developed to visually represent affective states in the embedding plots. This system uses a circular color wheel encoding emotional states as hues by angle, combined with radial saturation to indicate intensity.</p> <ul> <li>The color wheel covers 0\u00b0 to 360\u00b0 angles, mapped to specific colors representing emotional states (e.g., green for joy, red for anger, purple for frustration).</li> <li>Radial distance from the center reflects affect intensity, with desaturation toward white for neutral or low intensity.</li> <li>Behavioral valence and arousal labels are interpolated to the EEG sampling frequency and used to generate corresponding RGB colors.</li> </ul> <p>The embedding points were colored according to the valence-arousal color encoding from the behavioural labels, producing an interpretable affective latent space trajectory. </p> <p><pre><code>colors = valence_arousal_emotion_color(valence_aligned, arousal_aligned)\ncolors_hex = np.array([mcolors.to_hex(c) for c in colors])\nfig = plot_embedding_interactive(\n    embedding_small,\n    embedding_labels=colors_hex,\n    title=fig_title,\n    markersize=2,\n)\n</code></pre> Additional explorative plots for valence-arousal distributions and angle histograms are saved per configuration. </p> <p>e.g sub 005  </p>"},{"location":"methods/Phase_1/#next-step","title":"Next Step:","text":"<ul> <li>Multi-session shared embedding Include additional subjects to assess cross-subject variability and potential alignment in the embedding space.</li> <li>Multimodal embedding Incorporate behavioral or event-based labels to train supervised embeddings using time-aligned contrasts.</li> <li>Quantify embedding quality using metrics like clustering, decoding accuracy, or neighborhood preservation.</li> </ul>"},{"location":"methods/Phase_2/","title":"Multi-Session Shared Embedding","text":""},{"location":"methods/Phase_2/#overview","title":"Overview","text":"<p>Building upon the initial single-subject analysis, this phase extends the embedding framework to a multi-subject, multi-session setting. The goal is to learn a shared neural embedding space that generalizes across subjects, capturing common latent brain state dynamics while accounting for individual variability.The core objective of this step is to train a shared neural embedding model using CEBRA\u2019s multi-session framework. The model learns to represent neural activity from multiple subjects/sessions in a shared low-dimensional embedding space, enabling cross-subject generalization.</p>"},{"location":"methods/Phase_2/#data-loading-and-preprocessing","title":"Data Loading and Preprocessing","text":"<p>analog to Initial Exploration </p>"},{"location":"methods/Phase_2/#multi-session-dataset-construction","title":"Multi-Session Dataset Construction","text":"<p>The final step in data preparation involves creating a unified multi-session dataset using the CEBRA framework's specialized data structures:</p> <ol> <li> <p>TensorDataset Construction: Each subject's neural data is encapsulated within a <code>TensorDataset</code> object, which serves as CEBRA's core single-session data container. The TensorDataset class provides several key features:</p> <ul> <li>Neural Data Storage: Accepts neural activity arrays of shape (N, D) where N represents time points and D represents the number of channels</li> <li>Continuous Indexing: Time indices are stored as continuous behavioral variables of shape (N, d), enabling temporal contrastive learning</li> <li>Data Type Management: Automatically converts numpy arrays to PyTorch tensors and ensures appropriate data types (float32 for neural data, float for continuous indices)</li> <li>Device Compatibility: Supports GPU acceleration through device specification</li> <li>Flexible Indexing: Supports both discrete and continuous auxiliary variables for different types of behavioral correlates</li> </ul> </li> <li> <p>DatasetCollection Architecture: Individual TensorDataset objects are aggregated into a <code>DatasetCollection</code>, which implements CEBRA's multi-session dataset interface:</p> <ul> <li>Session Management: Maintains ordered collections of single-session datasets while preserving individual session characteristics</li> <li>Cross-Session Consistency: Validates that all sessions use consistent indexing schemes (all continuous or all discrete)</li> <li>Unified Access: Provides consolidated access to neural data and behavioral indices across all sessions through concatenated index tensors</li> <li>Device Synchronization: Ensures all constituent datasets reside on the same computational device</li> <li>Dimension Compatibility: Validates input dimensions across sessions for consistent model training</li> </ul> </li> </ol>"},{"location":"methods/Phase_2/#continuous-time-index-creation","title":"Continuous Time Index Creation","text":"<p>To enable temporal alignment across multiple recording sessions, a continuous time indexing system was implemented through the <code>create_time_index()</code> function. This approach:</p> <ul> <li>Generates sequential time indices for each subject's recording session</li> <li>Assumes a default sampling rate of 500 Hz (adjustable based on actual recording parameters)</li> <li>Creates time vectors in seconds, formatted as (n_samples, 1) arrays for compatibility with downstream analysis</li> </ul>"},{"location":"methods/Phase_2/#data-structure-and-format","title":"Data Structure and Format","text":"<p>The resulting dataset architecture leverages CEBRA's specialized data containers:</p> <ul> <li>Neural Data: Multi-dimensional tensors with dimensions (time_points, channels) for each subject, stored as float32 PyTorch tensors within TensorDataset objects</li> <li>Temporal Indices: Continuous time vectors stored as behavioral variables, enabling cross-session temporal alignment and contrastive learning</li> <li>Multi-Session Format: A DatasetCollection containing multiple TensorDataset instances, each representing an individual subject's recording session</li> <li>Index Concatenation: The DatasetCollection automatically concatenates continuous indices across all sessions, creating unified temporal representations for multi-session analysis</li> <li>Session Preservation: While enabling cross-session analysis, the architecture maintains individual session boundaries and characteristics through the underlying TensorDataset structure</li> </ul>"},{"location":"methods/Phase_2/#training-setup","title":"Training Setup","text":""},{"location":"methods/Phase_2/#model","title":"Model","text":"<p>A shared multi-layer neural network model is instantiated using CEBRA\u2019s model initialization utility. The model contains:</p> <ul> <li>An input layer matching the input dimension (number of neurons/channels)</li> <li>A hidden layer with 256 units</li> <li>An output embedding layer with a user-specified dimension (<code>output_dim</code>), representing the learned low-dimensional neural embedding space</li> </ul> <pre><code>model = cebra.models.init(\n    name=\"offset10-model\",\n    num_neurons=input_dim,\n    num_units=256,\n    num_output=output_dim\n).to(device)\n</code></pre>"},{"location":"methods/Phase_2/#dataloader-setup","title":"Dataloader Setup","text":"<p>The training process begins by creating a multi-session dataloader, MultiSessionLoader, which orchestrates sampling of contrastive data pairs (anchor, positive, and negative examples) uniformly across all sessions contained in the DatasetCollection. The excisting dataloader from CEBRA was modified. The original implementation of sample_conditional was designed with the assumption that each session is trained independently or with session-specific sub-models. This method ensured that the number of positive pairs per session remained equal, which was important in models with per-session encoders, where each session\u2019s representation was learned independently or in parallel.</p> <p>When using a shared model across all sessions (e.g. a unified encoder for all session data), this design introduces several problems:</p> <ol> <li>Artificial Session Separation: By limiting each query to search for a positive only within a single session, it ignores that similar states may occur across sessions, especially in aligned or synchronized data (e.g., behavioral or neural signals). This enforces a false boundary between sessions, preventing the model from learning truly shared representations.</li> <li>No Cross-Session Alignment: Cross-session samples that occur at the same logical time (e.g., time-aligned across subjects or trials) are not used as positives, even though they are often semantically equivalent. This misses an opportunity for temporal and semantic alignment across sessions \u2014 a key motivation for shared encoders.</li> <li>Over-constrained Sampling: The fixed within-session search restricts the variety of positive samples, which can lead to overfitting to session-specific patterns instead of learning invariant features.</li> </ol> <p>To address these issues , the dataloader was redesigned to facilitates contrastive learning by:</p> <ul> <li>Anchor samples: reference points from neural activity</li> <li>Positive samples: Either time-shifted (same-session) or cross-session (same-time). Session-specific or cross-session, based on random choice</li> <li>Negative samples: activity from other subjects/sessions to provide contrast</li> </ul> <p>The loader continuously samples batches with these triplets during training, ensuring uniform representation of subjects to avoid bias toward any particular session.</p> <p>The sampled indices are returned as a BatchIndex containing: - reference, positive, and negative indices for contrastive training. - index and index_reversed tensors used to align and mix positive pairs correctly during inference.</p> <p>This setup allows efficient construction of contrastive batches across sessions, supporting temporal and subject variability. With 50% probability, the positive is selected as:</p> <ul> <li>Same-session: nearest neighbor of the query (preserves temporal continuity)</li> <li>Cross-session: exact same time point in a randomly selected other session (enables alignment across subjects/sessions) </li> </ul> <p>The pairing is handled per sample, ensuring uniform mixing of intra- and inter-session positives. </p>"},{"location":"methods/Phase_2/#solver-setup","title":"Solver Setup","text":"<p>At the core of training is the MultiSessionSolver, a specialized training engine registered under the \"multi-session\" variant in CEBRA. This solver integrates model inference, loss computation, and parameter updates across all sessions in a unified framework. Core Responsibilities of the Solver - Parameter Management: The solver iterates over all model and criterion parameters for gradient updates. Since a shared model is used across sessions, parameters correspond to this single unified network. - Batch Processing &amp; Inference: For each batch of sampled triplets (reference, positive, negative), the solver computes feature embeddings by forwarding the samples through the shared model. It applies a mixing operation (_mix) to reorder positive samples according to indices, aligning reference-positive pairs correctly. - The training uses the FixedCosineInfoNCE loss function, a cosine similarity-based contrastive loss tailored for embedding learning. This loss encourages the model to maximize similarity between positive pairs while minimizing similarity with negative pairs, controlled by a temperature hyperparameter that sharpens or smooths the distribution of similarities. - The optimizer of choice is Adam, configured with the specified learning rate, to update model weights based on gradient descent.</p> <pre><code>solver = cebra.solver.init(\n    name=\"multi-session\",\n    model=model,\n    criterion=FixedCosineInfoNCE(temperature=1.0),\n    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate),\n    tqdm_on=True\n)\n</code></pre>"},{"location":"methods/Phase_2/#training-execution-and-output","title":"Training Execution and Output","text":"<p>The training loop was run via <code>solver.fit()</code>, managing data batching, forward passes, loss computation, and backpropagation.</p>"},{"location":"methods/Phase_2/#output","title":"Output","text":"<p>Upon successful training, the function returns a dictionary containing:</p> <ul> <li>Trained model: The learned embedding model instance</li> <li>Solver instance: Contains training state and parameters for further analysis or fine-tuning</li> </ul> <p>[Disclaimer!] The Cebra code had to be modified to enable this workflow a summary of the modifcations can be found in the MODIFICATIONS.md docuemtnation in the git repository</p>"},{"location":"methods/overview/","title":"Initial Exploration of EEG Embeddings","text":"<p>The goal phase of this project focused on exploring neural embeddings learned from preprocessed EEG data using the CEBRA (Contrastive Embedding for Behavioral and neural Representation Analysis) framework. </p> <p>This project consists of two main stages focusing on learning meaningful neural embeddings from EEG data:</p> <ol> <li> <p>Initial Exploration: This stage involves unsupervised learning of neural embeddings from preprocessed EEG recordings of a single subject using the CEBRA framework. The goal is to understand the latent structure of EEG signals and investigate effects such as channel selection and temporal dynamics in a low-dimensional embedding space.</p> </li> <li> <p>Advanced Modeling: The second stage will build upon the initial findings by developing a more complex model incorporating multi-subject and/or multimodal data. This phase aims to learn shared neural representations that generalize across sessions and subjects</p> </li> </ol>"},{"location":"results/cross-subject/","title":"Inital Results","text":"<p>This chapter presents a preliminary analysis of embeddings derived from CEBRA across different subjects and EEG channel configurations. The goal is to understand how neural variability is distributed in latent space and how different preprocessing choices impact the structure of the embeddings. The full interactive results can be viewed by running the dashboard.py script.</p>"},{"location":"results/cross-subject/#single-subjects-embeddings","title":"single subjects embeddings","text":"<p>We first examine the embeddings from all subjects using all available EEG channels on the same snippet of 60s.</p> <p>e.g sub 007 </p>"},{"location":"results/cross-subject/#interpreson-differencess","title":"Interpreson differencess","text":"<ul> <li>manifold complexity</li> <li>manifold compactness (more or less dispersion per line)</li> </ul>"},{"location":"results/cross-subject/#round-surface-coils","title":"Round surface &amp; Coils","text":"<p>When utilizing all available EEG channels, CEBRA produced a distinctive crescent curved mainfold structure with:</p> <ul> <li>holes </li> <li>cuttoff regions</li> </ul> <p>The spreading accross the whole surfacesuggests that each video elicits a wide range of neural states, not just one \u201cemotion point\u201d in the embedding space. Or the integration of signals across multiple brain regions, capturing both emotion-relevant and emotion-irrelevant neural variance.</p>"},{"location":"results/cross-subject/#colour-gradient-temporal-encoding","title":"Colour Gradient (Temporal Encoding)","text":"<p>The emotional labels, representing different points on the valence-arousal scale, showed extensive intermixing throughout the manifold structure. Rather than forming discrete clusters corresponding to emotional categories, different emotional states occupied overlapping regions within the embedding space.</p>"},{"location":"results/cross-subject/#single-segments","title":"Single segments","text":"<p>no to little effect on the manifold e.g sub 007 </p>"},{"location":"results/cross-subject/#frequency-band-filtering","title":"Frequency band filtering","text":"<p>e.g sub 007 </p>"},{"location":"results/cross-subject/#channels-selection","title":"Channels Selection","text":"<p>Because participants engaged in multiple simultaneous tasks (e.g., monitoring devices, labeling videos), neural activity related to these non-emotional cognitive processes might be mixed into the EEG signal. Removing or selecting specific channels can help filter out neural signals related to attention, motor activity, or other cognitive demands that are not central to the emotional processing we want to isolate. To est if certain brain regions drive specific embedding geometries drive embedding structures, multiple channel subsets were tested. e.g sub 007 </p> <p>restrictions to frontal, central, and parietal EEG channels:  - flattened, two-dimensional surface - improvement in emotional differentiation</p> <p>Different emotional categories that were previously intermixed became more spatially separated and distinguishable within the simplified manifold structure.</p> <ul> <li>Regional Specificity of Emotional Signals: The clearer emotional differentiation in frontal-central-parietal regions indicates that these brain areas contain more discriminable neural signatures for different emotional states.</li> </ul>"},{"location":"results/cross-subject/#assumption-emotion-dominates","title":"Assumption: emotion dominates","text":"<p>Participants watch a sequence of videos, each designed to elicit different emotions.</p>"},{"location":"results/cross-subject/#expectation","title":"Expectation","text":"<p>What we expected:  - Distinct clusters = emotionally distincs states     - embeddings form clear clusters, and each cluster corresponds to a particular video (or emotion). Sugessting that the brain enters qualitatively different neural states depending on the emotional content. The emotional processing is strong enough to produce separable neural dynamics. - Smooth Trajectories = emotional transitions     - embeddings form curved lines with temporal color gradient that form loops. Emotional stated evolve gradually and do not switch abrubtly. mixing and transitional states. continuous emotion space (e.g valence-arrousal)</p>"},{"location":"results/cross-subject/#results","title":"Results","text":"<p>What we got: </p>"},{"location":"results/cross-subject/#control-colouring-with-valence-arrousal-scores","title":"Control: Colouring with Valence Arrousal scores","text":"<p> - hue = different affective states - saturation = intensity</p>"},{"location":"results/exploration/","title":"Explorative Analyses","text":"<p>The exploratory analyses presented here aim to reveal underlying patterns, quantify variability, and assess the reliability and structure of the data before proceeding with deeper modeling or hypothesis testing.</p> <p></p> <p> </p> <p></p> <p>e.g sub 001 </p> <p> </p>"},{"location":"theory/cebra-introduction/","title":"Introduction to CEBRA","text":""},{"location":"theory/cebra-introduction/#what-is-cebra","title":"What is CEBRA?","text":"<p>CEBRA (Consistent EmBeddings of high-dimensional Recordings using Auxiliary variables) is a machine learning framework designed to discover meaningful low-dimensional representations of high-dimensional neural data. Unlike traditional dimensionality reduction techniques, CEBRA leverages auxiliary variables\u2014such as behavioral states, task conditions, or temporal information\u2014to guide the learning process toward biologically meaningful embeddings.</p>"},{"location":"theory/cebra-introduction/#core-principles","title":"Core Principles","text":"<p>The contrastive objective maximizes mutual information between neural embeddings and auxiliary variables.The InfoNCE loss provides a lower bound on this mutual information, making CEBRA a principled approach to information-maximizing representation learning. The learned embedding space exhibits specific geometric properties:</p> <ol> <li>Manifold alignment: Neural manifolds across sessions become aligned in the embedding space</li> <li>Clustering properties: Neural states with similar auxiliary variables form coherent clusters</li> <li>Smoothness: Continuous auxiliary variables induce smooth trajectories in embedding space</li> </ol>"},{"location":"theory/cebra-introduction/#theoretical-foundation","title":"Theoretical foundation","text":"<p>Given high-dimensional neural recordings \\(X \\in \\mathbb{R}^{N \\times D}\\) where \\(N\\) is the number of time points and \\(D\\) is the dimensionality (e.g., number of channels), and auxiliary variables \\(Y \\in \\mathbb{R}^{N \\times A}\\) encoding behavioral, temporal, or experimental information. </p> <p>CEBRA seeks to learn a mapping function: \\(f_\\theta: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d\\) where \\(d \\ll D\\) and \\(\\theta\\) represents learnable parameters. The objective is to construct embeddings \\(Z = f_\\theta(X)\\) that preserve relationships defined by the auxiliary variables while maintaining consistency across different recording sessions or experimental conditions.</p>"},{"location":"theory/cebra-introduction/#1-auxiliary-variable-guided-representation-learning","title":"1. Auxiliary Variable-Guided Representation Learning","text":"<p>Traditional unsupervised dimensionality reduction techniques (PCA, t-SNE, UMAP) optimize for variance preservation or local neighborhood structure without consideration of task-relevant information. CEBRA fundamentally differs by incorporating auxiliary variables \\(y_i\\) that encode biologically or behaviorally meaningful information about each neural observation \\(x_i\\).</p> <p>The auxiliary variables can take various forms:</p> <ul> <li>Continuous behavioral variables: Position trajectories, velocity, acceleration</li> <li>Discrete state variables: Task phases, decision outcomes, stimulus categories</li> <li>Temporal variables: Time indices, sequence positions, event markers</li> <li>Multi-modal auxiliary data: Simultaneously recorded behavioral measurements </li> </ul>"},{"location":"theory/cebra-introduction/#1-contrastive-learning-framework","title":"1. Contrastive Learning Framework","text":"<p>CEBRA operates on the principle of contrastive learning, where the algorithm learns to:</p> <ul> <li>Positive Pairs: Bring together neural activity patterns that share similar auxiliary variables </li> <li>Negative Pairs: Seperate neural activity patterns with different auxiliary variables </li> <li>Preserve the underlying structure of the data while reducing dimensionality</li> </ul> <p>This ensures that the learned representations capture the aspects of neural activity that are most relevant to the auxiliary variables of interest. (EXPLAIN FURTHER)</p>"},{"location":"theory/cebra-introduction/#2-consistency-across-sessions-explain-better","title":"2. Consistency Across Sessions (EXPLAIN BETTER)","text":"<p>The \"Consistent\" in CEBRA refers to the framework's ability to learn representations that are stable and comparable across different recording sessions, subjects, or experimental conditions. This is achieved through:</p> <ul> <li>Shared embedding spaces: Multiple datasets can be mapped to the same low-dimensional space</li> <li>Aligned representations: Similar neural states across sessions occupy similar positions in the embedding space</li> <li>Robust feature extraction: The learned features generalize across different data sources</li> </ul>"},{"location":"theory/cebra-introduction/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"theory/cebra-introduction/#embedding-objective","title":"Embedding Objective","text":"<p>CEBRA learns a mapping function \\(f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d\\) where \\(D\\) is the high-dimensional neural data space and \\(d\\) is the low-dimensional embedding space (\\(d &lt;&lt; D\\)).</p> <p>The objective function combines:</p> \\[\\mathcal{L} = \\mathcal{L}_{contrastive} + \\lambda_{cons} \\mathcal{L}_{consistency} + \\lambda_{reg} \\mathcal{L}_{regularization} \\] <p>Where: - \\(\\mathcal{L}_{contrastive}\\) ensures that similar auxiliary variables lead to similar embeddings - \\(\\mathcal{L}_{consistency}\\) maintains stability across different sessions or conditions - \\(\\mathcal{L}_{regularization}\\) INSERT WHAT and  \\(\\lambda_{cons}\\) and \\(\\lambda_{reg}\\) are weighting hyperparameters.</p>"},{"location":"theory/cebra-introduction/#contrastive-loss-formulation","title":"Contrastive Loss Formulation","text":"<p>The contrastive loss is based on the InfoNCE (Noise Contrastive Estimation) framework. For a reference sample \\(x_i\\) with auxiliary variable \\(y_i\\), the loss is:</p> \\[\\mathcal{L}_{contrastive} = -\\mathbb{E}_{i} \\left[ \\log \\frac{\\exp(s(z_i, z_i^+) / \\tau)}{\\exp(s(z_i, z_i^+) / \\tau) + \\sum_{k=1}^{K} \\exp(s(z_i, z_k^-) / \\tau)} \\right]\\] <p>where: - \\(z_i = f_\\theta(x_i)\\) is the embedding of the reference sample - \\(z_i^+ = f_\\theta(x_j)\\) where \\(x_j\\) is a positive sample (similar auxiliary variable) - \\(z_k^-\\) are embeddings of \\(K\\) negative samples (dissimilar auxiliary variables) - \\(s(\\cdot, \\cdot)\\) is a similarity function (typically cosine similarity or dot product) - \\(\\tau &gt; 0\\) is the temperature parameter controlling the concentration of the distribution. </p>"},{"location":"theory/cebra-introduction/#positive-and-negative-sampling-strategies","title":"Positive and Negative Sampling Strategies","text":"<p>The effectiveness of contrastive learning critically depends on the sampling strategy:</p> <ul> <li>Positive Sampling: </li> </ul> <p>For continuous auxiliary variables, positive samples are selected within a neighborhood: \\(\\(\\mathcal{N}_\\epsilon(y_i) = \\{y_j : \\|y_i - y_j\\| \\leq \\epsilon \\}\\)\\)</p> <p>For discrete variables, positive samples share the same categorical label.</p> <ul> <li>Negative Sampling:  Negative samples are selected to ensure diversity<ul> <li>Hard negative mining: Selecting challenging negative samples that are close in neural space but distant in auxiliary space</li> <li>Balanced sampling: Ensuring representation across different auxiliary variable values</li> <li>Temporal separation: For time-series data, maintaining sufficient temporal distance</li> </ul> </li> </ul>"},{"location":"theory/cebra-introduction/#consistency-loss","title":"Consistency Loss","text":"<p>The consistency term ensures alignment across different recording sessions or subjects. Let \\(\\{X^{(s)}, Y^{(s)}\\}_{s=1}^S\\) represent data from \\(S\\) different sessions. The consistency loss can be formulated as:</p> \\[\\mathcal{L}_{consistency} = \\sum_{s,s'=1}^S \\mathbb{E}_{i,j} \\left[ \\|f_\\theta(x_i^{(s)}) - f_\\theta(x_j^{(s')})\\|^2 \\cdot \\mathbf{1}[d(y_i^{(s)}, y_j^{(s')}) &lt; \\delta] \\right]\\] <p>where \\(\\mathbf{1}[\\cdot]\\) is the indicator function and \\(\\delta\\) defines the threshold for auxiliary variable similarity across sessions.</p>"},{"location":"theory/cebra-introduction/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>CEBRA typically employs deep neural networks as the encoder stability</p>"},{"location":"theory/cebra-introduction/#limitations-and-theoretical-challenges","title":"Limitations and Theoretical Challenges","text":""},{"location":"theory/cebra-introduction/#auxiliary-variable-dependence","title":"Auxiliary Variable Dependence","text":"<p>CEBRA's performance critically depends on: - Quality of auxiliary variables: Noisy or irrelevant auxiliary data degrades performance - Completeness: Missing auxiliary variables can lead to suboptimal embeddings - Temporal alignment: Misalignment between neural and auxiliary data affects learning</p>"},{"location":"theory/cebra-introduction/#data-requirements","title":"Data Requirements:","text":"<ul> <li>Auxiliary variables: Requires well-defined behavioral or temporal variables</li> <li>Sample size: Needs sufficient data for robust contrastive learning</li> <li>Data quality: Sensitive to noise in both neural and auxiliary data</li> </ul>"},{"location":"theory/cebra-introduction/#computational-considerations","title":"Computational Considerations:","text":"<ul> <li>Training time: Can be computationally intensive for large datasets</li> <li>Memory requirements: High-dimensional data can be memory-intensive</li> </ul>"},{"location":"theory/cebra-introduction/#interpretation-challenges","title":"Interpretation Challenges:","text":"<ul> <li>Embedding dimensions: Understanding what each dimension represents</li> <li>Nonlinear relationships: Complex mappings can be difficult to interpret</li> <li>Auxiliary variable choice: Results depend heavily on the quality of auxiliary variables</li> </ul>"},{"location":"theory/cebra-introduction/#conclusion","title":"Conclusion","text":"<p>CEBRA represents a theoretically grounded approach to neural representation learning that addresses fundamental challenges in neuroscience data analysis. By combining contrastive learning with auxiliary variable integration and consistency constraints, CEBRA provides a principled framework for discovering meaningful low-dimensional representations of complex neural dynamics. The mathematical foundation ensures both theoretical rigor and practical effectiveness, making CEBRA a powerful tool for understanding neural computation and behavior.</p>"},{"location":"theory/eeg_data/","title":"Why CEBRA for EEG Analysis?","text":"<p>Raw EEG data has: - High dimensionality: 64 channels - Temporal resolution: high sampling rate (number of samples per second) - Complex spatial patterns: Interactions between different brain regions</p> <p>This creates challenges: - Visualization: Impossible to directly visualize high-dimensional spaces - Interpretation: Difficult to understand patterns in high dimensions - Computation: Increased computational complexity</p>"},{"location":"theory/eeg_data/#advantages","title":"Advantages","text":"<ol> <li>Temporal resolution: High sampling rates capture fast neural dynamics</li> <li>Spatial coverage: Multiple electrodes provide simultaneous recordings</li> <li>Functional relevance: EEG signals reflect large-scale brain activity</li> <li>Task sensitivity: Clear relationships between brain states and behavior</li> </ol>"},{"location":"theory/eeg_data/#challenges","title":"Challenges","text":"<ol> <li>Volume conduction: Signals from different brain regions mix at electrodes</li> <li>Reference effects: Choice of reference electrode affects all channels</li> <li>Artifacts: Eye movements, muscle activity, and electrical interference</li> <li>Individual differences: Anatomical and functional variations between subjects</li> </ol> <p>When analyzing EEG data from multiple subjects, we face:</p> <ul> <li>Different anatomies: Varying brain structures and electrode positions</li> <li>Individual differences: Unique patterns of brain activity</li> <li>Alignment challenges: How to compare states across subjects?</li> </ul>"},{"location":"theory/eeg_data/#cebra-with-eeg-data","title":"CEBRA with EEG Data","text":""},{"location":"theory/eeg_data/#1-temporal-structure-preservation","title":"1. Temporal Structure Preservation","text":"<p>EEG data has rich temporal dynamics that traditional methods (UMAP, PCA)  cannot capture. CEBRA's ability to use temporal auxiliary variables makes it particularly suitable for:</p> <ul> <li>State transition analysis: Understanding how brain states evolve over time</li> <li>Sequence learning: Capturing temporal dependencies in neural activity</li> <li>Event-related dynamics: Linking neural responses to specific task events</li> </ul>"},{"location":"theory/eeg_data/#2-multi-subject-generalization","title":"2. Multi-Subject Generalization","text":"<p>In a multi-session EEG analysis, CEBRA's consistency principle enables:</p> <ul> <li>Cross-subject comparisons: Identifying shared neural patterns across individuals which helpt with the understanding of common brain dynamics</li> <li>Individual difference analysis: Quantifying how subjects differ in their neural responses</li> </ul>"},{"location":"theory/eeg_data/#biological-interpretability","title":"Biological Interpretability","text":"<p>The embeddings learned by CEBRA might correspond to:</p> <ul> <li>Neural population states: Coherent patterns of activity across neurons</li> <li>Behavioral modes: Different phases of task execution</li> <li>Cognitive states: Attention, memory, decision-making processes</li> <li>Temporal dynamics: State transitions and sequence information</li> </ul> <p>This theoretical foundation provides the basis for understanding how CEBRA can reveal shared neural dynamics across a multi-session EEG recordings, enabling insights into the common patterns of brain activity that underlie cognitive and behavioral processes.</p>"},{"location":"theory/neural-state-space/","title":"Neural State Space Analysis","text":""},{"location":"theory/neural-state-space/#conceptual-framework","title":"Conceptual Framework","text":"<p>Neural state space analysis is a mathematical framework for understanding brain activity as trajectories through a high-dimensional space, where each dimension represents the activity of a neural population or brain region. In the context of EEG analysis, this approach treats the electrical activity recorded from different electrodes as coordinates in a multi-dimensional space. EEG recordings capture electrical activity from multiple brain regions simultaneously, giving us a window into these dynamic brain states</p>"},{"location":"theory/neural-state-space/#state-space","title":"State Space","text":"<p>A neural state at time is defined as the pattern of activity across all recorded neural units. The different activity patterns of the brain can respond to:  - Attention states - Decision-making states - Motor preparation states - Cognitive load states</p>"},{"location":"theory/neural-state-space/#neural-dynamics-as-trajectories","title":"Neural Dynamics as Trajectories","text":"<p>Brain activity over time can be viewed as a trajectory  through the state space. This trajectory captures: - Instantaneous states: Brain activity at specific moments - Transitions: How the brain moves between different states - Temporal structure: The ordered sequence of states over time</p>"},{"location":"theory/neural-state-space/#discrete-vs-continuous-states","title":"Discrete vs. Continuous States","text":"<p>Neural states can be conceptualized as:</p> <ol> <li>Discrete states: Distinct, stable patterns of activity</li> <li>Decision states, attention states, motor preparation states</li> <li>Characterized by clustering in state space</li> <li> <p>Transitions between states are relatively rapid</p> </li> <li> <p>Continuous states: Smoothly varying patterns of activity</p> </li> <li>Gradual changes in attention or arousal</li> <li>Characterized by smooth trajectories in state space</li> <li>Transitions are gradual and continuous</li> </ol>"},{"location":"theory/neural-state-space/#dimensionality-and-manifold-structure","title":"Dimensionality and Manifold Structure","text":""},{"location":"theory/neural-state-space/#neural-manifolds","title":"Neural Manifolds","text":"<p>Despite the high dimensionality of neural data, neural activity could lie on lower-dimensional manifolds:</p> <p>This manifold structure arises because: - Correlated activity: Neural populations act together - Functional constraints: Brain activity serves specific computational purposes - Anatomical connectivity: Physical connections limit possible activity patterns</p>"},{"location":"theory/neural-state-space/#shared-manifold","title":"Shared Manifold","text":"<p>The shared manifold hypothesis suggests that despite individual differences, there exists a common low-dimensional space that captures: -Universal neural computations: Common brain processes across individuals -Task-related dynamics: Shared patterns of state transitions -Cognitive states: Common modes of brain activity</p> <p>This theoretical framework provides the foundation for understanding how neural state space analysis can reveal the shared patterns of brain activity across a multi-session EEG recordings, enabling insights into the common cognitive and neural processes that underlie human behavior.</p>"}]}