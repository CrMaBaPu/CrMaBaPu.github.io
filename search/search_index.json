{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CEBRA neural latent embeddings","text":"<p><code>[Last update: September 16, 2024]</code></p> <pre><code>Period:     2025-06 - 2025-08\nStatus:     in progress\n\nAuthor(s):  Cristina Maria Bayer\nContact:    bayer-cristina@t-online.de\n</code></pre>"},{"location":"#links","title":"Links","text":""},{"location":"#github-repository","title":"GitHub Repository","text":"<p>The GitHub Repository with all the code can be assessed here: github.com</p>"},{"location":"#documentation","title":"Documentation","text":"<p>The Documentation can be assessed here: mkdocs</p>"},{"location":"#project-description","title":"Project description","text":"<p>This project focuses on applying CEBRA (Contrastive Embedding for Behavior and Representation Analysis) to EEG data collected during a 20-minute immersive VR experience from the AVR Project. The dataset includes neural responses to a sequence of different videos shown in VR. The primary goal is to implement and explore the application of CEBRA for generating neural embeddings from EEG signals to better understand patterns and representations associated with different video stimuli. Preprocessed EEG data was provided by the AVR Project, allowing for direct experimentation with the CEBRA framework.</p>"},{"location":"#project-specific-challenges","title":"Project-Specific Challenges","text":"<p>Complex Stimulus Structure:</p> <ul> <li>Habituation Effects: Neural responses to 'b' may change across repetitions</li> <li>Context Effects: Same 'b' video may evoke different responses depending on preceding content</li> <li>Memory Interference: Later 'b' presentations may activate memory traces from earlier ones</li> <li>Expectation Effects: Participants may learn the pattern and anticipate repeated videos</li> </ul> <p>Continuous Labeling Interference:</p> <ul> <li>Cognitive Load: Dual-task demands affect natural emotional responses</li> <li>Motor Artifacts: Hand movements for labeling create systematic EEG noise</li> <li>Attention Division: Split attention between content and introspective monitoring</li> <li>Response Bias: The labeling process may alter the emotional experience itself</li> </ul> <p>Temporal Alignment Complexity:</p> <ul> <li>Multiple overlapping temporal structures (video timing, emotional dynamics, labeling timing, neural timing)</li> <li>Different frequency bands have different temporal characteristics</li> <li>Emotional responses may lag or persist beyond stimulus boundaries</li> <li>Individual differences in emotional response timing</li> </ul>"},{"location":"#experimental-design-strengths","title":"Experimental Design Strengths","text":"<p>Built-in Validation Structure:</p> <ul> <li>Repeated 'b' videos provide natural ground truth for embedding consistency</li> <li>Enable assessment of habituation vs. consistency in neural responses</li> <li>Allow detection of context effects (same stimulus, different preceding context)</li> <li>Serve as quality control for artifact removal effectiveness</li> </ul> <p>Rich Temporal Dynamics:</p> <ul> <li>Continuous emotional experience rather than discrete trials</li> <li>Natural emotional flow and transition patterns</li> <li>Captures emotional persistence and carryover effects</li> <li>Realistic emotional dynamics with overlapping responses</li> </ul> <p>Multi-dimensional Emotional Space:</p> <ul> <li>Different videos likely span various emotional dimensions</li> <li>Enables exploration of emotional transition dynamics</li> <li>Captures individual differences in emotional response patterns</li> <li>Provides diverse content for manifold discovery</li> </ul>"},{"location":"architecture/data-flow/","title":"Data Loading and Preprocessing","text":""},{"location":"architecture/data-flow/#dataset-overview","title":"Dataset Overview","text":"<p>This study utilized electroencephalography (EEG) data from multiple subjects stored in a standardized BIDS-compatible format. The dataset consists of preprocessed EEG recordings from individual subjects, with each subject's data stored in separate directories following the naming convention <code>sub-XXX</code>.</p>"},{"location":"architecture/data-flow/#eeg-data-loading","title":"EEG Data Loading","text":""},{"location":"architecture/data-flow/#subject-level-data-loading","title":"Subject-Level Data Loading","text":"<p>A custom data loading pipeline was implemented to handle both preprocessed (.fif) and raw (.edf) EEG file formats. The <code>load_subject()</code> function serves as the core loading mechanism, accepting a subject directory path and data type specification. For this analysis, preprocessed data in MNE-Python FIF format was utilized, specifically targeting files with the naming pattern <code>*before_ica.fif</code>, indicating data that had undergone preprocessing but prior to independent component analysis.</p>"},{"location":"architecture/data-flow/#multi-subject-data-integration","title":"Multi-Subject Data Integration","text":"<p>The <code>load_subjects()</code> function orchestrates the loading of multiple subjects with several key optimizations:</p> <ul> <li> <p>Channel Selection: To reduce computational complexity and focus on regions of interest, analysis was restricted to four midline electrodes: Fz (frontal), Cz (central), Pz (parietal), and Oz (occipital). This electrode selection provides coverage across major cortical regions while maintaining computational efficiency.</p> </li> <li> <p>Temporal Downsampling: To further optimize memory usage and processing speed, EEG signals were downsampled by a factor of 2, reducing the effective sampling rate while preserving essential signal characteristics.</p> </li> <li> <p>Memory Management: The pipeline incorporates explicit memory management through garbage collection and data type optimization (float32 precision) to handle large multi-subject datasets efficiently.</p> </li> </ul>"},{"location":"architecture/data-flow/#temporal-alignment-and-indexing","title":"Temporal Alignment and Indexing","text":""},{"location":"architecture/data-flow/#continuous-time-index-creation","title":"Continuous Time Index Creation","text":"<p>To enable temporal alignment across multiple recording sessions, a continuous time indexing system was implemented through the <code>create_time_index()</code> function. This approach:</p> <ul> <li>Generates sequential time indices for each subject's recording session</li> <li>Assumes a default sampling rate of 500 Hz (adjustable based on actual recording parameters)</li> <li>Creates time vectors in seconds, formatted as (n_samples, 1) arrays for compatibility with downstream analysis</li> </ul>"},{"location":"architecture/data-flow/#multi-session-dataset-construction","title":"Multi-Session Dataset Construction","text":"<p>The final step in data preparation involves creating a unified multi-session dataset using the CEBRA framework's specialized data structures:</p> <ol> <li> <p>TensorDataset Construction: Each subject's neural data is encapsulated within a <code>TensorDataset</code> object, which serves as CEBRA's core single-session data container. The TensorDataset class provides several key features:</p> <ul> <li>Neural Data Storage: Accepts neural activity arrays of shape (N, D) where N represents time points and D represents the number of channels</li> <li>Continuous Indexing: Time indices are stored as continuous behavioral variables of shape (N, d), enabling temporal contrastive learning</li> <li>Data Type Management: Automatically converts numpy arrays to PyTorch tensors and ensures appropriate data types (float32 for neural data, float for continuous indices)</li> <li>Device Compatibility: Supports GPU acceleration through device specification</li> <li>Flexible Indexing: Supports both discrete and continuous auxiliary variables for different types of behavioral correlates</li> </ul> </li> <li> <p>DatasetCollection Architecture: Individual TensorDataset objects are aggregated into a <code>DatasetCollection</code>, which implements CEBRA's multi-session dataset interface:</p> <ul> <li>Session Management: Maintains ordered collections of single-session datasets while preserving individual session characteristics</li> <li>Cross-Session Consistency: Validates that all sessions use consistent indexing schemes (all continuous or all discrete)</li> <li>Unified Access: Provides consolidated access to neural data and behavioral indices across all sessions through concatenated index tensors</li> <li>Device Synchronization: Ensures all constituent datasets reside on the same computational device</li> <li>Dimension Compatibility: Validates input dimensions across sessions for consistent model training</li> </ul> </li> </ol>"},{"location":"architecture/data-flow/#data-structure-and-format","title":"Data Structure and Format","text":"<p>The resulting dataset architecture leverages CEBRA's specialized data containers:</p> <ul> <li>Neural Data: Multi-dimensional tensors with dimensions (time_points, channels) for each subject, stored as float32 PyTorch tensors within TensorDataset objects</li> <li>Temporal Indices: Continuous time vectors stored as behavioral variables, enabling cross-session temporal alignment and contrastive learning</li> <li>Multi-Session Format: A DatasetCollection containing multiple TensorDataset instances, each representing an individual subject's recording session</li> <li>Index Concatenation: The DatasetCollection automatically concatenates continuous indices across all sessions, creating unified temporal representations for multi-session analysis</li> <li>Session Preservation: While enabling cross-session analysis, the architecture maintains individual session boundaries and characteristics through the underlying TensorDataset structure</li> </ul>"},{"location":"architecture/model-config/","title":"Model Training","text":""},{"location":"architecture/model-config/#overview","title":"Overview","text":"<p>The core objective of this step is to train a shared neural embedding model using CEBRA\u2019s multi-session framework. The model learns to represent neural activity from multiple subjects/sessions in a shared low-dimensional embedding space, enabling cross-subject generalization.</p>"},{"location":"architecture/model-config/#model-creation","title":"Model Creation","text":""},{"location":"architecture/model-config/#input-dimension-estimation","title":"Input Dimension Estimation","text":"<p>The model input dimension is determined from the first session in the dataset collection, ensuring the model architecture matches the data dimensionality. This input dimension corresponds to the number of neural features (e.g., channels or neurons) in each time step.</p>"},{"location":"architecture/model-config/#architecture","title":"Architecture","text":"<p>A shared multi-layer neural network model is instantiated using CEBRA\u2019s model initialization utility. The model contains:</p> <ul> <li>An input layer matching the input dimension (number of neurons/channels)</li> <li>A hidden layer with 256 units</li> <li>An output embedding layer with a user-specified dimension (<code>output_dim</code>), representing the learned low-dimensional neural embedding space</li> </ul> <pre><code>model = cebra.models.init(\n    name=\"offset10-model\",\n    num_neurons=input_dim,\n    num_units=256,\n    num_output=output_dim\n).to(device)\n</code></pre>"},{"location":"architecture/model-config/#dataset-configuration","title":"Dataset Configuration","text":"<p>To ensure compatibility between the dataset and model, the dataset collection is configured to align with the model\u2019s input expectations. This step prepares the dataset for effective training by sampling appropriate positive and negative examples across sessions.</p>"},{"location":"architecture/model-config/#dataloader-setup","title":"Dataloader Setup","text":"<p>The training process begins by creating a multi-session dataloader, MultiSessionLoader, which orchestrates sampling of contrastive data pairs (anchor, positive, and negative examples) uniformly across all sessions contained in the DatasetCollection. This dataloader facilitates contrastive learning by:</p> <ul> <li>Anchor samples: reference points from neural activity</li> <li>Positive samples: temporally nearby activity from the same subject/session</li> <li>Negative samples: activity from other subjects/sessions to provide contrast</li> </ul> <p>The loader continuously samples batches with these triplets during training, ensuring uniform representation of subjects to avoid bias toward any particular session.</p> <p>The sampled indices are returned as a BatchIndex containing: - reference, positive, and negative indices for contrastive training. - index and index_reversed tensors used to align and mix positive pairs correctly during inference.</p> <p>This setup allows efficient construction of contrastive batches across sessions, supporting temporal and subject variability. </p>"},{"location":"architecture/model-config/#loss-function-and-optimization","title":"Loss Function and Optimization","text":"<p>At the core of training is the MultiSessionSolver, a specialized training engine registered under the \"multi-session\" variant in CEBRA. This solver integrates model inference, loss computation, and parameter updates across all sessions in a unified framework. Core Responsibilities of the Solver - Parameter Management: The solver iterates over all model and criterion parameters for gradient updates. Since a shared model is used across sessions, parameters correspond to this single unified network.</p> <ul> <li>Batch Processing &amp; Inference: For each batch of sampled triplets (reference, positive, negative), the solver computes feature embeddings by forwarding the samples through the shared model. It applies a mixing operation (_mix) to reorder positive samples according to indices, aligning reference-positive pairs correctly.</li> </ul> <p>The training uses the FixedCosineInfoNCE loss function, a cosine similarity-based contrastive loss tailored for embedding learning. This loss encourages the model to maximize similarity between positive pairs while minimizing similarity with negative pairs, controlled by a temperature hyperparameter that sharpens or smooths the distribution of similarities.</p> <p>The optimizer of choice is Adam, configured with the specified learning rate, to update model weights based on gradient descent.</p> <pre><code>solver = cebra.solver.init(\n    name=\"multi-session\",\n    model=model,\n    criterion=FixedCosineInfoNCE(temperature=1.0),\n    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate),\n    tqdm_on=True\n)\n</code></pre>"},{"location":"architecture/model-config/#training-execution","title":"Training Execution","text":"<p>The training loop is executed by calling the solver\u2019s <code>.fit()</code> method, which internally manages batch retrieval, forward passes through the model, loss computation, and weight updates. </p>"},{"location":"architecture/model-config/#output","title":"Output","text":"<p>Upon successful training, the function returns a dictionary containing:</p> <ul> <li>Trained model: The learned embedding model instance</li> <li>Solver instance: Contains training state and parameters for further analysis or fine-tuning</li> </ul>"},{"location":"architecture/overview/","title":"Initial Exploration of EEG Embeddings","text":"<p>The initial phase of this project focused on exploring neural embeddings learned from preprocessed EEG data using the CEBRA (Contrastive Embedding for Behavioral and neural Representation Analysis) framework. This step served as an unsupervised, exploratory analysis to evaluate the latent structure of neural activity and its variability depending on the selected EEG channels and subject identity.</p>"},{"location":"architecture/overview/#objectives","title":"Objectives","text":"<ul> <li>Test the CEBRA embedding framework on preprocessed EEG recordings.</li> <li>Investigate how the resulting embeddings vary across different channel subsets.</li> <li>Visualize the temporal dynamics of EEG representations in a reduced latent space.</li> <li>Establish a foundation for future extension and comparisons across subjects or conditions.</li> </ul>"},{"location":"architecture/overview/#_1","title":"System Overview","text":"<ul> <li>Channel Effects: Preliminary visual differences were examined when using different sets of EEG channels (e.g., frontal vs. parietal)</li> <li>Subject-Specific Embedding: Only one subject was analyzed in this stage. Generalization across subjects will be tested in subsequent experiments.</li> </ul>"},{"location":"architecture/overview/#data-preparation","title":"Data Preparation","text":"<p>Preprocessed EEG data were loaded for a single subject (e.g <code>sub-020</code>) from a cleaned dataset directory. The data had already undergone necessary preprocessing (e.g., filtering, artifact removal, ICA), and therefore represent neural activity in a relatively clean and interpretable form.</p> <p>Three channel configurations were defined to examine the spatial dependence of the embeddings:</p> <ul> <li>All Channels: Full set of EEG, EOG, and ECG channels.</li> <li>Frontal and Frontotemporal Channels: To isolate activity from cognitive and executive function regions.</li> <li>Parietal Channels: Targeting posterior activity, often associated with visual and sensorimotor processing.</li> </ul> <p>Only EEG channels were used during model fitting. Auxiliary channels (e.g., EOG, ECG) were excluded from the embedding process.</p> <pre><code>picks = mne.pick_types(raw.info, eeg=True, eog=False)\ndata = raw.get_data(picks=picks).T  # shape: (n_times, n_channels)\n</code></pre> <p>The time series data were transposed and concatenated to form a matrix of shape <code>(n_timepoints, n_channels)</code>, suitable for input to the CEBRA model.</p>"},{"location":"architecture/overview/#embedding-via-cebra","title":"Embedding via CEBRA","text":"<p>The CEBRA model was initialized with the following configuration:</p> <ul> <li>Model architecture: Offset-based temporal contrastive model (<code>offset10-model</code>)</li> <li>Training mode: Unsupervised (<code>conditional=None</code>)</li> <li>Distance metric: Cosine similarity</li> <li>Latent dimensionality: 3 (for visualization)</li> <li>Hardware: GPU used if available</li> </ul> <pre><code>cebra_model = CEBRA(\n    model_architecture=\"offset10-model\",\n    batch_size=512,\n    learning_rate=3e-4,\n    temperature_mode='constant',\n    temperature=1.12,\n    max_iterations=5000,\n    conditional=None,\n    output_dimension=3,\n    distance='cosine',\n    device=\"cuda_if_available\",\n)\n</code></pre> <p>After training, the latent representation of the EEG time series was computed:</p> <pre><code>embedding = cebra_model.transform(X)\n</code></pre>"},{"location":"architecture/overview/#visualization","title":"Visualization","text":"<p>The embeddings were visualized using an interactive 3D scatter plot, with time used as the color gradient to illustrate temporal progression of brain states.</p> <p>Due to the large number of timepoints, a downsampling step was applied before visualization to improve rendering performance and clarity:</p> <pre><code>idx = np.linspace(0, len(times) - 1, 10000).astype(int)\nembedding_small = embedding[idx]\ntimes_small = times[idx]\n</code></pre> <p>The resulting plot displays the trajectory of brain states over time in the learned latent space:</p> <pre><code>fig = plot_embedding_interactive(\n    embedding_small,\n    embedding_labels=times_small,\n    title=\"CEBRA Embedding (Brain States)\",\n    markersize=5,\n    cmap=\"rainbow\"\n)\n</code></pre>"},{"location":"architecture/overview/#next-step","title":"Next Step:","text":"<ul> <li>Multi-session shared embedding Include additional subjects to assess cross-subject variability and potential alignment in the embedding space.</li> <li>Multimodal embeddingIncorporate behavioral or event-based labels to train supervised embeddings using time-aligned contrasts.</li> <li>Quantify embedding quality using metrics like clustering, decoding accuracy, or neighborhood preservation.</li> </ul>"},{"location":"architecture/overview/#multi-session-shared-embedding","title":"Multi-Session shared embedding","text":"<p>Great! Here\u2019s a clean, organized draft section for your Multi-Session Shared Embedding part based on what you described. It fits well as a continuation of your initial exploration and sets the stage for cross-subject analysis using CEBRA.</p>"},{"location":"architecture/overview/#multi-session-shared-embedding_1","title":"Multi-Session Shared Embedding","text":""},{"location":"architecture/overview/#overview","title":"Overview","text":"<p>Building upon the initial single-subject analysis, this phase extends the embedding framework to a multi-subject, multi-session setting. The goal is to learn a shared neural embedding space that generalizes across subjects, capturing common latent brain state dynamics while accounting for individual variability.</p>"},{"location":"architecture/overview/#data-loading-and-preprocessing","title":"Data Loading and Preprocessing","text":"<p>analog to Initial Exploration</p>"},{"location":"architecture/overview/#temporal-alignment-and-dataset-construction","title":"Temporal Alignment and Dataset Construction","text":"<ul> <li>A continuous time indexing system (<code>create_time_index()</code>) aligned EEG samples across sessions, generating sequential time vectors (in seconds) compatible with downstream analysis.</li> <li>Each subject's data was encapsulated within a <code>TensorDataset</code>, CEBRA\u2019s single-session data container holding neural data <code>(N, D)</code> and continuous time indices <code>(N, 1)</code>.</li> <li>These datasets were combined into a <code>DatasetCollection</code>, which manages multi-session data by maintaining session boundaries and ensuring consistent indexing formats.</li> <li>The multi-session structure enables unified access to all subjects\u2019 data while preserving session-specific features.</li> </ul>"},{"location":"architecture/overview/#model-architecture-and-training-setup","title":"Model Architecture and Training Setup","text":"<ul> <li>The input dimension was determined by the number of channels (4 electrodes), ensuring proper model-data compatibility.</li> <li> <p>A shared neural network was initialized with:</p> </li> <li> <p>An input layer matching the EEG channel count.</p> </li> <li>A hidden layer with 256 units.</li> <li>An output embedding layer with a user-defined dimensionality (<code>output_dim</code>).</li> </ul> <pre><code>model = cebra.models.init(\n    name=\"offset10-model\",\n    num_neurons=input_dim,\n    num_units=256,\n    num_output=output_dim\n).to(device)\n</code></pre> <ul> <li> <p>A multi-session dataloader (<code>MultiSessionLoader</code>) was configured to sample contrastive triplets uniformly across sessions, including:</p> </li> <li> <p>Anchor samples: reference EEG time points.</p> </li> <li>Positive samples: temporally nearby points within the same session.</li> <li>Negative samples: points from other sessions/subjects.</li> <li>embeddings that capture both within-subject temporal continuity and across-subject variability.</li> </ul>"},{"location":"architecture/overview/#loss-function-and-optimization","title":"Loss Function and Optimization","text":"<ul> <li>Training leveraged CEBRA\u2019s MultiSessionSolver, which integrates multi-session batch processing, model inference, and gradient updates.</li> <li>The FixedCosineInfoNCE loss encouraged high similarity between positive pairs and low similarity with negative pairs, using a cosine distance metric and temperature scaling.</li> <li>Optimization was performed using Adam with a predefined learning rate.</li> </ul> <pre><code>solver = cebra.solver.init(\n    name=\"multi-session\",\n    model=model,\n    criterion=FixedCosineInfoNCE(temperature=1.0),\n    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate),\n    tqdm_on=True\n)\n</code></pre>"},{"location":"architecture/overview/#training-execution-and-output","title":"Training Execution and Output","text":"<ul> <li>The training loop was run via <code>solver.fit()</code>, managing data batching, forward passes, loss computation, and backpropagation.</li> <li> <p>Upon completion, the method returned:</p> </li> <li> <p>The trained embedding model representing shared latent neural dynamics.</p> </li> <li>The solver instance for tracking training progress and enabling further analysis or fine-tuning.</li> </ul>"},{"location":"architecture/overview/#summary","title":"Summary","text":"<p>This multi-session shared embedding approach enables cross-subject comparison of EEG brain states within a unified latent space, paving the way for analyses of inter-subject variability, group-level clustering, and transfer learning across recording sessions.</p>"},{"location":"components/dataset-collection/","title":"CEBRA Multi-Session Architecture Modifications","text":""},{"location":"components/dataset-collection/#summary","title":"Summary","text":"<p>This document outlines critical modifications made to the CEBRA (Consistent EmBeddings of high-dimensional Recordings using Auxiliary variables) codebase to resolve training failures and implement a single shared model architecture for multi-session datasets. The changes address two primary issues: missing required arguments in dataset dimension handling and iterator compatibility problems in the solver model.</p>"},{"location":"components/dataset-collection/#issues-addressed","title":"Issues Addressed","text":""},{"location":"components/dataset-collection/#issue-1-datasetcollection-input-dimension-error","title":"Issue 1: DatasetCollection Input Dimension Error","text":"<p>Error: <code>Training failed: DatasetCollection.get_input_dimension() missing 1 required positional argument: 'session_id'</code></p>"},{"location":"components/dataset-collection/#issue-2-model-iterator-compatibility-error","title":"Issue 2: Model Iterator Compatibility Error","text":"<p>Error: <code>Training failed: 'Offset10Model' object is not iterable</code></p>"},{"location":"components/dataset-collection/#modifications-overview","title":"Modifications Overview","text":""},{"location":"components/dataset-collection/#1-datasetcollection-class-cebra_datamultisessiondataset","title":"1. DatasetCollection Class (<code>cebra_data.MultiSessionDataset</code>)","text":""},{"location":"components/dataset-collection/#problem","title":"Problem","text":"<p>The original implementation required a <code>session_id</code> parameter for <code>get_input_dimension()</code> but the property <code>input_dimension</code> didn't provide one, causing training failures.</p>"},{"location":"components/dataset-collection/#solution","title":"Solution","text":"<p>Modified the class to provide both a property fallback and an optional parameter method:</p> <pre><code>@property\ndef input_dimension(self):\n    # Return the input dimension of the first session as fallback\n    return self._datasets[0].input_dimension\n\ndef get_input_dimension(self, session_id: Optional[int] = None) -&gt; int:\n    \"\"\"Get the feature dimension of the required session.\n\n    Args:\n        session_id: The session ID, an integer between 0 and\n            :py:attr:`num_sessions`. If None, returns first session dimension.\n\n    Returns:\n        A single session input dimension for the requested session id.\n    \"\"\"\n    if session_id is None:\n        # fallback to first session input dimension\n        return self._datasets[0].input_dimension\n    return self.get_session(session_id).input_dimension\n</code></pre>"},{"location":"components/dataset-collection/#changes-made","title":"Changes Made","text":"<ul> <li>Added: Property <code>input_dimension</code> that returns the first session's input dimension as a fallback</li> <li>Modified: <code>get_input_dimension()</code> method to accept optional <code>session_id</code> parameter</li> <li>Behavior: When <code>session_id</code> is <code>None</code>, returns the first session's input dimension; otherwise, returns the specified session's dimension</li> </ul>"},{"location":"components/dataset-collection/#2-multisessionloader-class","title":"2. MultiSessionLoader Class","text":""},{"location":"components/dataset-collection/#problem_1","title":"Problem","text":"<p>The solver expected the loader to have an <code>input_dimension</code> property, but <code>MultiSessionLoader</code> didn't implement this property.</p>"},{"location":"components/dataset-collection/#solution_1","title":"Solution","text":"<p>Added the missing property to delegate to the dataset:</p> <pre><code>@property\ndef input_dimension(self):\n    return self.dataset.input_dimension\n</code></pre>"},{"location":"components/dataset-collection/#changes-made_1","title":"Changes Made","text":"<ul> <li>Added: <code>input_dimension</code> property that delegates to <code>self.dataset.input_dimension</code></li> <li>Purpose: Provides consistent interface for dimension access across different loader types</li> </ul>"},{"location":"components/dataset-collection/#3-solver-architecture-single-model-implementation","title":"3. Solver Architecture: Single Model Implementation","text":""},{"location":"components/dataset-collection/#problem_2","title":"Problem","text":"<p>The original implementation treated <code>self.model</code> as an iterable collection of models (one per session), but the goal was to use a single shared model across all sessions.</p>"},{"location":"components/dataset-collection/#solution_2","title":"Solution","text":"<p>Refactored multiple methods to treat <code>self.model</code> as a single model instance rather than a collection:</p>"},{"location":"components/dataset-collection/#31-parameters-method","title":"3.1 Parameters Method","text":"<pre><code>def parameters(self, session_id: Optional[int] = None):\n    \"\"\"Iterate over all parameters.\n\n    Args:\n        session_id: The session ID, an :py:class:`int` between 0 and\n            the number of sessions -1 for multisession, and set to\n            ``None`` for single session.\n\n    Yields:\n        The parameters of the model.\n    \"\"\"\n    self._check_is_session_id_valid(session_id=session_id)\n\n    for parameter in self.model.parameters():\n        yield parameter\n\n    for parameter in self.criterion.parameters():\n        yield parameter\n</code></pre> <p>Change: Removed indexing <code>self.model[session_id]</code> and directly iterate over <code>self.model.parameters()</code></p>"},{"location":"components/dataset-collection/#32-inference-method","title":"3.2 Inference Method","text":"<pre><code>def _inference(self, batches: List[cebra.data.Batch]) -&gt; cebra.data.Batch:\n    \"\"\"Given batches of input examples, computes the feature representations/embeddings.\"\"\"\n    refs = []\n    poss = []\n    negs = []\n\n    for batch in batches:\n        batch.to(self.device)\n        refs.append(self.model(batch.reference))\n        poss.append(self.model(batch.positive))\n        negs.append(self.model(batch.negative))\n\n    ref = torch.stack(refs, dim=0)\n    pos = torch.stack(poss, dim=0)\n    neg = torch.stack(negs, dim=0)\n\n    pos = self._mix(pos, batches[0].index_reversed)\n    num_features = neg.shape[2]\n\n    return cebra.data.Batch(\n        reference=ref.view(-1, num_features),\n        positive=pos.view(-1, num_features),\n        negative=neg.view(-1, num_features),\n    )\n</code></pre> <p>Change: Removed <code>zip(batches, self.model)</code> pattern and use single <code>self.model</code> for all batches</p>"},{"location":"components/dataset-collection/#33-get-model-method","title":"3.3 Get Model Method","text":"<pre><code>def _get_model(self, session_id: Optional[int] = None):\n    \"\"\"Get the model for the given session ID.\"\"\"\n    self._check_is_session_id_valid(session_id=session_id)\n    self._check_is_fitted()\n    return self.model\n</code></pre> <p>Change: Return <code>self.model</code> directly instead of <code>self.model[session_id]</code></p>"},{"location":"components/dataset-collection/#34-validation-method","title":"3.4 Validation Method","text":"<pre><code>def validation(self, loader, session_id: Optional[int] = None):\n    \"\"\"Compute score of the model on data.\"\"\"\n    assert session_id is not None\n\n    iterator = self._get_loader(loader)\n    total_loss = Meter()\n    self.model.eval()\n    for _, batch in iterator:\n        prediction = self._single_model_inference(batch, self.model)\n        loss, _, _ = self.criterion(prediction.reference,\n                                    prediction.positive,\n                                    prediction.negative)\n        total_loss.add(loss.item())\n    return total_loss.average\n</code></pre> <p>Change: Use <code>self.model.eval()</code> and <code>self.model</code> instead of <code>self.model[session_id]</code></p>"},{"location":"components/dataset-collection/#implementation-guide","title":"Implementation Guide","text":"<p>To implement these changes in your CEBRA clone:</p>"},{"location":"components/dataset-collection/#step-1-locate-target-files","title":"Step 1: Locate Target Files","text":"<ul> <li>Find the <code>DatasetCollection</code> class in the multi-session dataset module</li> <li>Locate the <code>MultiSessionLoader</code> class in the data loading module</li> <li>Find the solver class containing the methods listed above</li> </ul>"},{"location":"components/dataset-collection/#step-2-apply-datasetcollection-changes","title":"Step 2: Apply DatasetCollection Changes","text":"<ol> <li>Replace the <code>input_dimension</code> property implementation</li> <li>Modify the <code>get_input_dimension</code> method signature and implementation</li> <li>Ensure proper import of <code>Optional</code> from <code>typing</code></li> </ol>"},{"location":"components/dataset-collection/#step-3-apply-multisessionloader-changes","title":"Step 3: Apply MultiSessionLoader Changes","text":"<ol> <li>Add the <code>input_dimension</code> property to the class</li> </ol>"},{"location":"components/dataset-collection/#step-4-apply-solver-changes","title":"Step 4: Apply Solver Changes","text":"<ol> <li>Update all four methods (<code>parameters</code>, <code>_inference</code>, <code>_get_model</code>, <code>validation</code>)</li> <li>Replace model indexing patterns with direct model usage</li> <li>Ensure consistent single-model treatment across all methods</li> </ol>"},{"location":"components/dataset-collection/#step-5-testing","title":"Step 5: Testing","text":"<ol> <li>Run multi-session training to verify the fixes resolve both error conditions</li> </ol>"},{"location":"components/dataset-collection/#technical-impact","title":"Technical Impact","text":""},{"location":"components/dataset-collection/#architectural-changes","title":"Architectural Changes","text":"<ul> <li>Model Sharing: Transition from per-session models to single shared model architecture</li> <li>Interface Consistency: Standardized dimension access patterns across loaders and datasets</li> </ul>"},{"location":"components/dataset-collection/#backward-compatibility","title":"Backward Compatibility","text":"<p>These changes maintain backward compatibility by:</p> <ul> <li>Preserving existing method signatures</li> <li>Providing fallback behaviors for optional parameters</li> <li>Maintaining expected return types and interfaces</li> </ul>"},{"location":"components/tensor-dataset/","title":"TensorDataset Design","text":""},{"location":"components/tensor-dataset/#conceptual-role","title":"Conceptual Role","text":"<p>The TensorDataset serves as the fundamental building block for organizing and accessing individual EEG recording sessions in your multi-session analysis. It acts as a bridge between raw neural data and the sophisticated machine learning framework of CEBRA, providing a structured way to handle the temporal and spatial complexity of EEG recordings.</p>"},{"location":"components/tensor-dataset/#core-design-philosophy","title":"Core Design Philosophy","text":""},{"location":"components/tensor-dataset/#1-unified-data-representation","title":"1. Unified Data Representation","text":"<p>TensorDataset creates a unified interface for neural data by:</p> <ul> <li>Standardizing formats: Converting various data formats (NumPy arrays, PyTorch tensors) into a consistent representation</li> <li>Handling heterogeneity: Accommodating different data types (continuous behavioral variables, discrete task labels)</li> <li>Maintaining relationships: Preserving the temporal alignment between neural activity and auxiliary variables</li> </ul>"},{"location":"components/tensor-dataset/#2-auxiliary-variable-integration","title":"2. Auxiliary Variable Integration","text":"<p>The dataset design explicitly supports auxiliary variables that guide the learning process:</p> <pre><code># Conceptual structure\nTensorDataset(\n    neural=eeg_data,        # Shape: (time_points, channels)\n    continuous=behavior,    # Shape: (time_points, behavioral_dims)\n    discrete=task_labels   # Shape: (time_points,)\n)\n</code></pre> <p>This tri-partite structure ensures that: - Neural data provides the high-dimensional observations - Continuous variables capture smooth behavioral changes (e.g., attention levels, arousal) - Discrete variables encode categorical information (e.g., task conditions, stimulus types)</p>"},{"location":"components/tensor-dataset/#data-structure-and-organization","title":"Data Structure and Organization","text":""},{"location":"components/tensor-dataset/#temporal-alignment","title":"Temporal Alignment","text":"<p>The dataset maintains strict temporal alignment across all data modalities:</p> \\[\\mathbf{X}(t) = \\begin{bmatrix} \\mathbf{x}_{neural}(t) \\\\ \\mathbf{x}_{continuous}(t) \\\\ \\mathbf{x}_{discrete}(t) \\end{bmatrix}\\] <p>Where: - \\(\\mathbf{x}_{neural}(t) \\in \\mathbb{R}^{N_{channels}}\\) is the EEG activity at time \\(t\\) - \\(\\mathbf{x}_{continuous}(t) \\in \\mathbb{R}^{N_{behavioral}}\\) are continuous behavioral variables - \\(\\mathbf{x}_{discrete}(t) \\in \\mathbb{Z}^{N_{categorical}}\\) are discrete task variables</p>"},{"location":"components/tensor-dataset/#offset-mechanism","title":"Offset Mechanism","text":"<p>The offset property enables temporal context extraction:</p> <pre><code>offset = Offset(left=5, right=5)  # 10 time points total\n</code></pre> <p>This creates temporal windows around each reference time point:</p> \\[\\mathbf{X}_{window}(t) = [\\mathbf{X}(t-5), \\mathbf{X}(t-4), \\ldots, \\mathbf{X}(t+4), \\mathbf{X}(t+5)]\\] <p>The offset mechanism serves multiple purposes:</p> <ol> <li>Temporal context: Captures neural dynamics leading up to and following events</li> <li>Noise reduction: Smooths out transient fluctuations</li> <li>Feature enrichment: Provides more information for pattern recognition</li> <li>Causal modeling: Enables analysis of cause-effect relationships</li> </ol>"},{"location":"components/tensor-dataset/#functional-capabilities","title":"Functional Capabilities","text":""},{"location":"components/tensor-dataset/#1-dynamic-indexing","title":"1. Dynamic Indexing","text":"<p>The <code>expand_index</code> method transforms simple time indices into rich temporal windows:</p> <pre><code># Simple index\nindex = [100, 150, 200]\n\n# Expanded with offset\nexpanded = dataset.expand_index(index)\n# Shape: (3, 11) for offset=5 left and right\n</code></pre> <p>This expansion enables: - Multi-scale analysis: Examining neural activity at different temporal scales - Context preservation: Maintaining information about surrounding time points - Flexible sampling: Adapting to different analysis requirements</p>"},{"location":"components/tensor-dataset/#2-data-type-validation","title":"2. Data Type Validation","text":"<p>The dataset enforces data type consistency:</p> <ul> <li>Neural data: Must be floating-point (typically float32 or float64)</li> <li>Continuous variables: Must be floating-point for smooth interpolation</li> <li>Discrete variables: Must be integer types for categorical encoding</li> </ul> <p>This validation prevents common errors and ensures compatibility with downstream analysis tools.</p>"},{"location":"components/tensor-dataset/#3-device-management","title":"3. Device Management","text":"<p>The dataset handles device placement for efficient computation:</p> <pre><code>dataset = TensorDataset(neural_data, device=\"cuda\")\n</code></pre> <p>This enables: - GPU acceleration: Faster computation for large datasets - Memory management: Efficient use of device memory - Seamless integration: Compatibility with PyTorch-based models</p>"},{"location":"components/tensor-dataset/#data-access-patterns","title":"Data Access Patterns","text":""},{"location":"components/tensor-dataset/#temporal-window-retrieval","title":"Temporal Window Retrieval","text":"<p>When accessing data, the dataset returns temporal windows rather than single time points:</p> <pre><code># Access single time point\ndata = dataset[t]\n# Returns shape: (channels, temporal_window)\n</code></pre> <p>This access pattern: - Preserves temporal structure: Maintains the sequential nature of neural data - Enables convolution: Compatible with convolutional neural networks - Supports temporal modeling: Facilitates analysis of temporal dependencies</p>"},{"location":"components/tensor-dataset/#batch-processing","title":"Batch Processing","text":"<p>The dataset supports efficient batch processing:</p> <pre><code># Batch access\nbatch_data = dataset[batch_indices]\n# Returns shape: (batch_size, channels, temporal_window)\n</code></pre> <p>Batch processing enables: - Parallel computation: Simultaneous processing of multiple samples - Memory efficiency: Optimal use of computational resources - Scalability: Handling large datasets efficiently</p>"},{"location":"components/tensor-dataset/#integration-with-cebra-framework","title":"Integration with CEBRA Framework","text":""},{"location":"components/tensor-dataset/#auxiliary-variable-utilization","title":"Auxiliary Variable Utilization","text":"<p>The dataset's auxiliary variables directly inform CEBRA's contrastive learning:</p> <ol> <li>Positive pairs: Samples with similar auxiliary variables</li> <li>Negative pairs: Samples with different auxiliary variables</li> <li>Reference samples: Anchor points for comparison</li> </ol>"},{"location":"components/tensor-dataset/#temporal-consistency","title":"Temporal Consistency","text":"<p>The offset mechanism ensures temporal consistency in the learned representations:</p> <ul> <li>Smooth transitions: Nearby time points have similar representations</li> <li>Temporal invariance: Representations are stable across small time shifts</li> <li>Causal relationships: Past context informs current representations</li> </ul>"},{"location":"components/tensor-dataset/#advantages-for-eeg-analysis","title":"Advantages for EEG Analysis","text":""},{"location":"components/tensor-dataset/#1-temporal-richness","title":"1. Temporal Richness","text":"<p>EEG data has complex temporal structure that the TensorDataset captures:</p> <ul> <li>Fast dynamics: Capturing millisecond-scale neural events</li> <li>Slow trends: Preserving longer-term changes in brain state</li> <li>Rhythmic patterns: Maintaining oscillatory components</li> </ul>"},{"location":"components/tensor-dataset/#2-multimodal-integration","title":"2. Multimodal Integration","text":"<p>The dataset naturally handles multiple data streams:</p> <ul> <li>Neural activity: High-dimensional EEG recordings</li> <li>Behavioral measures: Continuous performance metrics</li> <li>Task structure: Discrete experimental conditions</li> </ul>"},{"location":"components/tensor-dataset/#3-preprocessing-integration","title":"3. Preprocessing Integration","text":"<p>The dataset can incorporate preprocessing steps:</p> <ul> <li>Artifact removal: Filtering out noise and artifacts</li> <li>Normalization: Standardizing signal amplitudes</li> <li>Feature extraction: Computing derived measures</li> </ul>"},{"location":"components/tensor-dataset/#memory-and-computational-considerations","title":"Memory and Computational Considerations","text":""},{"location":"components/tensor-dataset/#memory-optimization","title":"Memory Optimization","text":"<p>The dataset implements several memory optimization strategies:</p> <ol> <li>Lazy loading: Loading data only when accessed</li> <li>Efficient storage: Using appropriate data types</li> <li>Device management: Optimal placement of data in memory</li> </ol>"},{"location":"components/tensor-dataset/#computational-efficiency","title":"Computational Efficiency","text":"<p>The design prioritizes computational efficiency:</p> <ol> <li>Vectorized operations: Using efficient tensor operations</li> <li>Batch processing: Minimizing loop overhead</li> <li>GPU compatibility: Leveraging parallel processing</li> </ol>"},{"location":"components/tensor-dataset/#limitations-and-design-trade-offs","title":"Limitations and Design Trade-offs","text":""},{"location":"components/tensor-dataset/#memory-requirements","title":"Memory Requirements","text":"<p>The temporal window approach requires more memory:</p> \\[\\text{Memory} = N_{samples} \\times N_{channels} \\times N_{timepoints} \\times \\text{dtype\\_size}\\] <p>Where \\(N_{timepoints}\\) is determined by the offset size.</p>"},{"location":"components/tensor-dataset/#temporal-resolution","title":"Temporal Resolution","text":"<p>The offset mechanism imposes constraints on temporal resolution:</p> <ul> <li>Fixed windows: All samples use the same temporal context</li> <li>Boundary effects: Samples near recording boundaries may be excluded</li> <li>Resolution trade-offs: Larger offsets capture more context but reduce temporal precision</li> </ul>"},{"location":"components/tensor-dataset/#future-enhancements","title":"Future Enhancements","text":""},{"location":"components/tensor-dataset/#adaptive-offsets","title":"Adaptive Offsets","text":"<p>Future versions might support adaptive offset sizes:</p> <ul> <li>Task-dependent: Different offsets for different task conditions</li> <li>Data-driven: Automatically determining optimal offset sizes</li> <li>Hierarchical: Multi-scale temporal analysis</li> </ul>"},{"location":"components/tensor-dataset/#advanced-preprocessing","title":"Advanced Preprocessing","text":"<p>Enhanced preprocessing capabilities could include:</p> <ul> <li>Real-time processing: Online artifact removal and normalization</li> <li>Adaptive filtering: Context-dependent signal processing</li> <li>Quality assessment: Automatic data quality evaluation</li> </ul> <p>The TensorDataset design provides a robust foundation for handling the complexity of EEG data while maintaining the flexibility needed for sophisticated neural state analysis. Its integration with auxiliary variables and temporal windowing makes it particularly well-suited for CEBRA's contrastive learning approach, enabling the discovery of meaningful patterns in neural activity across different subjects and experimental conditions.</p>"},{"location":"dataset/eeg-overview/","title":"EEG Data Overview","text":"<p>This descirption was taken from ARV Project. </p> <p>The dataset consists of 54 subjects each with a measurement of 20 minutes with a 64 channel EEG. </p>"},{"location":"dataset/eeg-overview/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<p>The EEG data processing follows a comprehensive pipeline that transforms raw LSL-output format (<code>.xdf</code>) files into analysis-ready datasets using BIDS-compatible structure. </p>"},{"location":"dataset/eeg-overview/#eeg-signal-processing","title":"EEG Signal Processing","text":""},{"location":"dataset/eeg-overview/#preprocessing-steps","title":"Preprocessing Steps","text":"<p>The EEG preprocessing pipeline implements a robust multi-stage approach to ensure high-quality neural signal extraction:</p> <p>Initial Processing: Raw EEG data undergoes cropping of 2.5 seconds from both start and end to eliminate edge artifacts that commonly occur during recording initiation and termination.</p> <p>Signal Cleaning: The cleaning process involves several critical steps. Power line interference is removed at 50 Hz, followed by re-referencing to the average across all channels to reduce common noise. Band-pass filtering between 0.1 and 45 Hz isolates the neurophysiologically relevant frequency range while removing low-frequency drift and high-frequency noise.</p> <p>Epoch Processing: The continuous EEG signal is segmented into 10-second epochs for systematic analysis. Bad channels and epochs are identified and either rejected or interpolated using the <code>autoreject</code> algorithm applied to 1 Hz-filtered data, ensuring optimal data quality while preserving maximum usable data.</p> <p>Artifact Removal: Independent Component Analysis (ICA) is employed to identify and remove physiological artifacts including eye movements, cardiac interference, and muscle activity that can contaminate neural signals.</p> <p>Quality Control: A final threshold check excludes participants with more than 30% noisy epochs remaining after preprocessing, ensuring only high-quality datasets proceed to analysis.</p>"},{"location":"dataset/eeg-overview/#feature-extraction","title":"Feature Extraction","text":"<p>The feature extraction process transforms preprocessed EEG signals into frequency-domain representations suitable for analysis:</p> <p>Signal Preparation: Data is resampled to 100 Hz for computational efficiency while preserving essential frequency information. Symmetric padding of 90 seconds of mirrored data at signal boundaries prevents edge artifacts during time-frequency decomposition.</p> <p>Time-Frequency Analysis: Continuous Wavelet Transform (CWT) using the <code>fCWT</code> library generates Time Frequency Representations (TFR), providing detailed information about spectral power changes over time.</p> <p>Temporal Averaging: Power values are averaged over 2-second windows with 50% overlap, balancing temporal resolution with statistical stability.</p> <p>Frequency Band Integration: The TFR is integrated across standard neurophysiological frequency bands (delta, theta, alpha, beta, gamma) to extract band-specific power measures that correspond to different cognitive and physiological states.</p> <p>Spatial Averaging: Finally, power values are averaged across anatomically defined regions of interest including posterior, frontal, and whole-brain regions, providing both localized and global measures of neural activity.</p> <p>This comprehensive preprocessing and feature extraction pipeline ensures that the EEG data maintains high signal quality while providing meaningful neurophysiological measures for subsequent analysis.</p>"},{"location":"dataset/eeg-overview/#vr-specific-challenges-for-eeg","title":"VR-Specific Challenges for EEG","text":"<p>Novel Artifact Sources:</p> <ul> <li>Head Movement Artifacts: VR headset weight and balance create constant micro-movements</li> <li>Visual-Induced Artifacts: Rapid eye movements tracking VR content differ from natural viewing</li> <li>Electromagnetic Interference: VR displays and tracking systems introduce novel noise patterns</li> <li>Comfort Artifacts: Progressive muscle tension and sweating under headset affect electrode impedance</li> </ul> <p>Technical Integration Issues:</p> <ul> <li>Electrode placement complicated by VR headset</li> <li>Potential electromagnetic interference between systems</li> <li>Synchronization challenges between VR events and EEG timing</li> <li>Limited head movement freedom affects natural viewing behavior</li> </ul> <p>Participant Experience Effects:</p> <ul> <li>VR sickness can confound emotional responses</li> <li>Novelty effects may overshadow target emotional content</li> <li>Individual differences in VR adaptation and comfort</li> <li>Dual-task interference from continuous emotional labeling</li> </ul>"},{"location":"dataset/multi-subject-design/","title":"Study Design","text":"<p>This study explores the relationship between affective experience and physiological responses during immersive virtual reality (VR) scenarios. The resulting dataset captures multimodal physiological and behavioral data in response to emotionally evocative VR stimuli.</p>"},{"location":"dataset/multi-subject-design/#overview","title":"Overview","text":"<p>The experimental pipeline consists of several sequential phases:</p> <ol> <li>Pre-Survey</li> <li>Setup</li> <li>Resting State (Pre)</li> <li>Training</li> <li>Experiment</li> <li>Resting State (Post)</li> <li>Post-Survey</li> </ol>"},{"location":"dataset/multi-subject-design/#1-pre-survey","title":"1. Pre-Survey","text":"<p>Participants begin by completing a pre-survey assessing their baseline mood, demographic information, and individual differences in emotional traits.</p>"},{"location":"dataset/multi-subject-design/#2-setup","title":"2. Setup","text":"<p>Participants are instrumented with a range of biosensors to monitor:</p> <ul> <li>EEG (electroencephalogram)</li> <li>ECG (electrocardiogram)</li> <li>PPG (photoplethysmogram)</li> <li>GSR (galvanic skin response)</li> <li>Respiration belt</li> <li>Eye tracking</li> </ul>"},{"location":"dataset/multi-subject-design/#3-resting-state-pre","title":"3. Resting State (Pre)","text":"<p>Participants undergo two resting-state conditions:</p> <ul> <li>Eyes Open</li> <li>Eyes Closed</li> </ul> <p>This baseline data provides a physiological benchmark for comparison with task-related responses.</p>"},{"location":"dataset/multi-subject-design/#4-training","title":"4. Training","text":"<p>Participants are introduced to the Flubber interface in VR, a dynamic rating tool used to continuously report their affective state (valence and arousal) during the experiment.</p> <ul> <li>Valence: Negative to Positive</li> <li>Arousal: Low to High</li> <li>The Flubber pulsates (slow/fast) and changes shape (regular/irregular) to reflect arousal and valence.</li> </ul>"},{"location":"dataset/multi-subject-design/#5-experiment","title":"5. Experiment","text":"<p>Participants experience a sequence of emotionally charged VR scenarios (e.g., spaceship, invasion, asteroids, underwood). During the entire session:</p> <ul> <li>They use a handheld VR controller to continuously rate their emotional state via the Flubber interface.</li> <li>At the end of the session, participants provide summary ratings for each scene.</li> </ul>"},{"location":"dataset/multi-subject-design/#6-resting-state-post","title":"6. Resting State (Post)","text":"<p>The resting state procedure is repeated (eyes open and closed) to observe any post-experiment physiological shifts.</p>"},{"location":"dataset/multi-subject-design/#7-post-survey","title":"7. Post-Survey","text":"<p>A final survey is completed, collecting subjective feedback and post-session emotional self-assessment.</p>"},{"location":"dataset/multi-subject-design/#data-description","title":"Data Description","text":"<p>The dataset derived from this experiment includes:</p> <ul> <li>Physiological Signals: EEG, ECG, PPG, GSR, respiration, and eye tracking</li> <li>VR Controller Data: Continuous Flubber ratings of valence and arousal</li> <li>Survey Data: Pre/post emotional states, participant demographics, and scene-specific ratings</li> </ul>"},{"location":"results/cross-subject/","title":"Inital Results","text":"<p>This chapter presents a preliminary analysis of embeddings derived from CEBRA across different subjects and EEG channel configurations. The goal is to understand how neural variability is distributed in latent space and how different preprocessing choices impact the structure of the embeddings.</p>"},{"location":"results/cross-subject/#single-subjects-embeddings","title":"single subjects embeddings","text":"<p>We first examine the embeddings from three subjects (e.g sub-001, sub-002, sub-020) using all available EEG channels.</p> <p>sub-001 | sub-002 | sub-020  </p>"},{"location":"results/cross-subject/#flat-shpere","title":"Flat  Shpere","text":"<p>The embeddings appear to lie on a curved but largely 2D surface (like a flattened sphere or disc) within 3D space.</p> <p>All subjects consistently display a \"flat sphere\" shape in their embeddings\u2014suggesting the data lie predominantly on a two-dimensional manifold within a three-dimensional space. This geometric constraint implies low intrinsic dimensionality of the neural data: most brain state variability can be captured by two dominant latent factors.</p>"},{"location":"results/cross-subject/#elongation","title":"Elongation","text":"<p>Elongation = Variance concentrated along one axis.</p> <p>Elongated embeddings (e.g.sub-002) indicate that neural variability is dominated by a single primary latent dimension.</p> <p>Rounder embeddings (e.g., sub-020) suggest a more balanced contribution from at least two latent dimensions.</p>"},{"location":"results/cross-subject/#colour-gradient-temporal-encoding","title":"Colour Gradient (Temporal Encoding)","text":"<p>A clear color gradient exists in all embeddings\u2014from red (early time points) to blue/purple (later time points). This gradient is radially distributed, with earlier points often clustered toward the center and later ones radiating outward. Two interpretations are considered:</p> <ul> <li>A technical property of CEBRA: embeddings may be initialized near the origin, then optimized to expand outward.</li> <li>participants might show higher focus or engagement early in the task</li> </ul> <p>Time does not dominate the embeddings\u2014there is no strict linear trajectory. Instead, time is diffusely spread, suggesting that non-temporal neural dynamics (e.g., emotional or cognitive processes) drive the main structure.  </p> <p>The even spreading accross the whole planeuggests that each video elicits a wide range of neural states, not just one \u201cemotion point\u201d in the embedding space. The brain state varies a lot within each video. Or, other cognitive factors (e.g., memory, attention, visual input) are interacting with the emotion, creating diverse brain activity.</p>"},{"location":"results/cross-subject/#channels-selection","title":"Channels Selection","text":"<p>Because participants engaged in multiple simultaneous tasks (e.g., monitoring devices, labeling videos), neural activity related to these non-emotional cognitive processes might be mixed into the EEG signal. Removing or selecting specific channels can help filter out neural signals related to attention, motor activity, or other cognitive demands that are not central to the emotional processing we want to isolate. To est if certain brain regions drive specific embedding geometries drive embedding structures, multiple channel subsets were tested in subject 020. </p> <p></p> <p>Across all channel subsets:</p> <ul> <li>The flatness and color gradient persist.</li> <li>The degree of elongation varies, suggesting some configurations emphasize specific latent dimensions more than others.</li> </ul> <p>Changes in the geomertry of the embedding suggest that different regions carry different types/amount of intormation about the neural state space</p>"},{"location":"results/cross-subject/#frontal-channels","title":"Frontal Channels","text":"<p>Subset: ['Fz', 'F3', 'F4', 'F7', 'F8', 'AF3', 'AF4', 'AFz']</p> <p>The resulting embeddings are more rounded, particularly in the center. Suggests a more balanced distribution of neural variability across dimensions</p>"},{"location":"results/cross-subject/#frontotemporal-channels","title":"Frontotemporal Channels","text":"<p>Frontal channels + T7, T8</p> <p>Embeddings become more elongated and thinner, with reduced spread. This suggests that variability is dominated by a single, strong latent factor\u2014likely tied to external sensory-emotional input. </p>"},{"location":"results/cross-subject/#frontal-parietal-channels","title":"Frontal-Parietal Channels","text":"<p>Subset: ['P3', 'P4', 'Pz']</p> <p>The resulting embeddings are distinctly round and evenly spread. </p>"},{"location":"results/cross-subject/#assumption-emotion-dominates","title":"Assumption: emotion dominates","text":"<p>Participants watch a sequence of videos, each designed to elicit different emotions (e.g., joy, sadness, fear, calm).</p>"},{"location":"results/cross-subject/#expectation","title":"Expectation","text":"<p>What we expected:  - Distinct clusters = emotionally distincs states     - embeddings form clear clusters, and each cluster corresponds to a particular video (or emotion). Sugessting that the brain enters qualitatively different neural states depending on the emotional content. The emotional processing is strong enough to produce separable neural dynamics. - Smooth Trajectories = emotional transitions     - embeddings form curved lines with temporal color gradient that form loops. Emotional stated evolve gradually and do not switch abrubtly. mixing and transitional states. continuous emotion space (e.g valence-arrousal)</p>"},{"location":"results/cross-subject/#results","title":"Results","text":"<p>What we got: Elongates shapes  - the emebdding is treched along one direction     - one primary emotional dimension dimunates (e.g either arousal or valence). brain's response scales along a principal dimension (e.g pleasantlness)</p> <p>What we got: Color Gradient - all videos explore the full space. points are scattered across the whole space     - Each video causes movement through both high and low arousal states and     - both positive and negative valence states.</p>"},{"location":"results/cross-subject/#alternatives","title":"Alternatives","text":"<p>Parietal channels, associated with attention and sensory integration, produced the roundest embeddings. This implies that fluctuations in attentional focus or sensory processing, which may vary within and across videos, contribute strongly to neural variability.</p> <p>Frontal and frontotemporal channels yielded elongated embeddings, which could reflect dominant latent factors related to task engagement, decision-making, or motor preparation rather than pure emotion.</p>"},{"location":"theory/cebra-introduction/","title":"Introduction to CEBRA","text":""},{"location":"theory/cebra-introduction/#what-is-cebra","title":"What is CEBRA?","text":"<p>CEBRA (Consistent EmBeddings of high-dimensional Recordings using Auxiliary variables) is a machine learning framework designed to discover meaningful low-dimensional representations of high-dimensional neural data. Unlike traditional dimensionality reduction techniques, CEBRA leverages auxiliary variables\u2014such as behavioral states, task conditions, or temporal information\u2014to guide the learning process toward biologically meaningful embeddings.</p>"},{"location":"theory/cebra-introduction/#core-principles","title":"Core Principles","text":"<p>The contrastive objective maximizes mutual information between neural embeddings and auxiliary variables.The InfoNCE loss provides a lower bound on this mutual information, making CEBRA a principled approach to information-maximizing representation learning. The learned embedding space exhibits specific geometric properties:</p> <ol> <li>Manifold alignment: Neural manifolds across sessions become aligned in the embedding space</li> <li>Clustering properties: Neural states with similar auxiliary variables form coherent clusters</li> <li>Smoothness: Continuous auxiliary variables induce smooth trajectories in embedding space</li> </ol>"},{"location":"theory/cebra-introduction/#theoretical-foundation","title":"Theoretical foundation","text":"<p>Given high-dimensional neural recordings \\(X \\in \\mathbb{R}^{N \\times D}\\) where \\(N\\) is the number of time points and \\(D\\) is the dimensionality (e.g., number of channels), and auxiliary variables \\(Y \\in \\mathbb{R}^{N \\times A}\\) encoding behavioral, temporal, or experimental information. </p> <p>CEBRA seeks to learn a mapping function: \\(f_\\theta: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d\\) where \\(d \\ll D\\) and \\(\\theta\\) represents learnable parameters. The objective is to construct embeddings \\(Z = f_\\theta(X)\\) that preserve relationships defined by the auxiliary variables while maintaining consistency across different recording sessions or experimental conditions.</p>"},{"location":"theory/cebra-introduction/#1-auxiliary-variable-guided-representation-learning","title":"1. Auxiliary Variable-Guided Representation Learning","text":"<p>Traditional unsupervised dimensionality reduction techniques (PCA, t-SNE, UMAP) optimize for variance preservation or local neighborhood structure without consideration of task-relevant information. CEBRA fundamentally differs by incorporating auxiliary variables \\(y_i\\) that encode biologically or behaviorally meaningful information about each neural observation \\(x_i\\).</p> <p>The auxiliary variables can take various forms:</p> <ul> <li>Continuous behavioral variables: Position trajectories, velocity, acceleration</li> <li>Discrete state variables: Task phases, decision outcomes, stimulus categories</li> <li>Temporal variables: Time indices, sequence positions, event markers</li> <li>Multi-modal auxiliary data: Simultaneously recorded behavioral measurements </li> </ul>"},{"location":"theory/cebra-introduction/#1-contrastive-learning-framework","title":"1. Contrastive Learning Framework","text":"<p>CEBRA operates on the principle of contrastive learning, where the algorithm learns to:</p> <ul> <li>Positive Pairs: Bring together neural activity patterns that share similar auxiliary variables </li> <li>Negative Pairs: Seperate neural activity patterns with different auxiliary variables </li> <li>Preserve the underlying structure of the data while reducing dimensionality</li> </ul> <p>This ensures that the learned representations capture the aspects of neural activity that are most relevant to the auxiliary variables of interest. (EXPLAIN FURTHER)</p>"},{"location":"theory/cebra-introduction/#2-consistency-across-sessions-explain-better","title":"2. Consistency Across Sessions (EXPLAIN BETTER)","text":"<p>The \"Consistent\" in CEBRA refers to the framework's ability to learn representations that are stable and comparable across different recording sessions, subjects, or experimental conditions. This is achieved through:</p> <ul> <li>Shared embedding spaces: Multiple datasets can be mapped to the same low-dimensional space</li> <li>Aligned representations: Similar neural states across sessions occupy similar positions in the embedding space</li> <li>Robust feature extraction: The learned features generalize across different data sources</li> </ul>"},{"location":"theory/cebra-introduction/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"theory/cebra-introduction/#embedding-objective","title":"Embedding Objective","text":"<p>CEBRA learns a mapping function \\(f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d\\) where \\(D\\) is the high-dimensional neural data space and \\(d\\) is the low-dimensional embedding space (\\(d &lt;&lt; D\\)).</p> <p>The objective function combines:</p> \\[\\mathcal{L} = \\mathcal{L}_{contrastive} + \\lambda_{cons} \\mathcal{L}_{consistency} + \\lambda_{reg} \\mathcal{L}_{regularization} \\] <p>Where: - \\(\\mathcal{L}_{contrastive}\\) ensures that similar auxiliary variables lead to similar embeddings - \\(\\mathcal{L}_{consistency}\\) maintains stability across different sessions or conditions - \\(\\mathcal{L}_{regularization}\\) INSERT WHAT and  \\(\\lambda_{cons}\\) and \\(\\lambda_{reg}\\) are weighting hyperparameters.</p>"},{"location":"theory/cebra-introduction/#contrastive-loss-formulation","title":"Contrastive Loss Formulation","text":"<p>The contrastive loss is based on the InfoNCE (Noise Contrastive Estimation) framework. For a reference sample \\(x_i\\) with auxiliary variable \\(y_i\\), the loss is:</p> \\[\\mathcal{L}_{contrastive} = -\\mathbb{E}_{i} \\left[ \\log \\frac{\\exp(s(z_i, z_i^+) / \\tau)}{\\exp(s(z_i, z_i^+) / \\tau) + \\sum_{k=1}^{K} \\exp(s(z_i, z_k^-) / \\tau)} \\right]\\] <p>where: - \\(z_i = f_\\theta(x_i)\\) is the embedding of the reference sample - \\(z_i^+ = f_\\theta(x_j)\\) where \\(x_j\\) is a positive sample (similar auxiliary variable) - \\(z_k^-\\) are embeddings of \\(K\\) negative samples (dissimilar auxiliary variables) - \\(s(\\cdot, \\cdot)\\) is a similarity function (typically cosine similarity or dot product) - \\(\\tau &gt; 0\\) is the temperature parameter controlling the concentration of the distribution. </p>"},{"location":"theory/cebra-introduction/#positive-and-negative-sampling-strategies","title":"Positive and Negative Sampling Strategies","text":"<p>The effectiveness of contrastive learning critically depends on the sampling strategy:</p> <ul> <li>Positive Sampling: </li> </ul> <p>For continuous auxiliary variables, positive samples are selected within a neighborhood: \\(\\(\\mathcal{N}_\\epsilon(y_i) = \\{y_j : \\|y_i - y_j\\| \\leq \\epsilon \\}\\)\\)</p> <p>For discrete variables, positive samples share the same categorical label.</p> <ul> <li>Negative Sampling:  Negative samples are selected to ensure diversity<ul> <li>Hard negative mining: Selecting challenging negative samples that are close in neural space but distant in auxiliary space</li> <li>Balanced sampling: Ensuring representation across different auxiliary variable values</li> <li>Temporal separation: For time-series data, maintaining sufficient temporal distance</li> </ul> </li> </ul>"},{"location":"theory/cebra-introduction/#consistency-loss","title":"Consistency Loss","text":"<p>The consistency term ensures alignment across different recording sessions or subjects. Let \\(\\{X^{(s)}, Y^{(s)}\\}_{s=1}^S\\) represent data from \\(S\\) different sessions. The consistency loss can be formulated as:</p> \\[\\mathcal{L}_{consistency} = \\sum_{s,s'=1}^S \\mathbb{E}_{i,j} \\left[ \\|f_\\theta(x_i^{(s)}) - f_\\theta(x_j^{(s')})\\|^2 \\cdot \\mathbf{1}[d(y_i^{(s)}, y_j^{(s')}) &lt; \\delta] \\right]\\] <p>where \\(\\mathbf{1}[\\cdot]\\) is the indicator function and \\(\\delta\\) defines the threshold for auxiliary variable similarity across sessions.</p>"},{"location":"theory/cebra-introduction/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>CEBRA typically employs deep neural networks as the encoder stability</p>"},{"location":"theory/cebra-introduction/#limitations-and-theoretical-challenges","title":"Limitations and Theoretical Challenges","text":""},{"location":"theory/cebra-introduction/#auxiliary-variable-dependence","title":"Auxiliary Variable Dependence","text":"<p>CEBRA's performance critically depends on: - Quality of auxiliary variables: Noisy or irrelevant auxiliary data degrades performance - Completeness: Missing auxiliary variables can lead to suboptimal embeddings - Temporal alignment: Misalignment between neural and auxiliary data affects learning</p>"},{"location":"theory/cebra-introduction/#data-requirements","title":"Data Requirements:","text":"<ul> <li>Auxiliary variables: Requires well-defined behavioral or temporal variables</li> <li>Sample size: Needs sufficient data for robust contrastive learning</li> <li>Data quality: Sensitive to noise in both neural and auxiliary data</li> </ul>"},{"location":"theory/cebra-introduction/#computational-considerations","title":"Computational Considerations:","text":"<ul> <li>Training time: Can be computationally intensive for large datasets</li> <li>Memory requirements: High-dimensional data can be memory-intensive</li> </ul>"},{"location":"theory/cebra-introduction/#interpretation-challenges","title":"Interpretation Challenges:","text":"<ul> <li>Embedding dimensions: Understanding what each dimension represents</li> <li>Nonlinear relationships: Complex mappings can be difficult to interpret</li> <li>Auxiliary variable choice: Results depend heavily on the quality of auxiliary variables</li> </ul>"},{"location":"theory/cebra-introduction/#conclusion","title":"Conclusion","text":"<p>CEBRA represents a theoretically grounded approach to neural representation learning that addresses fundamental challenges in neuroscience data analysis. By combining contrastive learning with auxiliary variable integration and consistency constraints, CEBRA provides a principled framework for discovering meaningful low-dimensional representations of complex neural dynamics. The mathematical foundation ensures both theoretical rigor and practical effectiveness, making CEBRA a powerful tool for understanding neural computation and behavior.</p>"},{"location":"theory/dimensionality-reduction/","title":"Why CEBRA for EEG Analysis?","text":"<p>Raw EEG data has: - High dimensionality: 64 channels - Temporal resolution: high number of samples per second</p> <p>This creates challenges: - Visualization: Impossible to directly visualize high-dimensional spaces - Interpretation: Difficult to understand patterns in high dimensions - Computation: Increased computational complexity</p>"},{"location":"theory/dimensionality-reduction/#advantages-for-eeg-data","title":"Advantages for EEG Data","text":"<ol> <li>Temporal resolution: High sampling rates capture fast neural dynamics</li> <li>Spatial coverage: Multiple electrodes provide simultaneous recordings</li> <li>Functional relevance: EEG signals reflect large-scale brain activity</li> <li>Task sensitivity: Clear relationships between brain states and behavior</li> </ol>"},{"location":"theory/dimensionality-reduction/#challenges-in-eeg-state-space-analysis","title":"Challenges in EEG State Space Analysis","text":"<ol> <li>Volume conduction: Signals from different brain regions mix at electrodes</li> <li>Reference effects: Choice of reference electrode affects all channels</li> <li>Artifacts: Eye movements, muscle activity, and electrical interference</li> <li>Individual differences: Anatomical and functional variations between subjects</li> </ol> <p>When analyzing EEG data from multiple subjects, we face:</p> <ul> <li>Different anatomies: Varying brain structures and electrode positions</li> <li>Individual differences: Unique patterns of brain activity</li> <li>Alignment challenges: How to compare states across subjects</li> </ul>"},{"location":"theory/dimensionality-reduction/#1-temporal-structure-preservation","title":"1. Temporal Structure Preservation","text":"<p>EEG data has rich temporal dynamics that traditional methods often struggle to capture. CEBRA's ability to use temporal auxiliary variables makes it particularly suitable for:</p> <ul> <li>State transition analysis: Understanding how brain states evolve over time</li> <li>Sequence learning: Capturing temporal dependencies in neural activity</li> <li>Event-related dynamics: Linking neural responses to specific task events</li> </ul>"},{"location":"theory/dimensionality-reduction/#2-multi-subject-generalization","title":"2. Multi-Subject Generalization","text":"<p>In your multi-session EEG analysis, CEBRA's consistency principle enables:</p> <ul> <li>Cross-subject comparisons: Identifying shared neural patterns across individuals</li> <li>Population-level insights: Understanding common brain dynamics</li> <li>Individual difference analysis: Quantifying how subjects differ in their neural responses</li> </ul>"},{"location":"theory/dimensionality-reduction/#3-high-dimensional-data-handling","title":"3. High-Dimensional Data Handling","text":"<p>EEG recordings typically involve: - Many channels: 64, 128, or more electrodes - High sampling rates: Thousands of samples per second - Complex spatial patterns: Interactions between different brain regions</p> <p>CEBRA efficiently handles this complexity by learning compact representations that preserve the most important information for your research questions.</p>"},{"location":"theory/dimensionality-reduction/#biological-interpretability","title":"Biological Interpretability","text":"<p>The embeddings learned by CEBRA often correspond to:</p> <ul> <li>Neural population states: Coherent patterns of activity across neurons</li> <li>Behavioral modes: Different phases of task execution</li> <li>Cognitive states: Attention, memory, decision-making processes</li> <li>Temporal dynamics: State transitions and sequence information</li> </ul> <p>This interpretability makes CEBRA particularly valuable for neuroscientific research, where understanding the biological meaning of the results is crucial. This theoretical foundation provides the basis for understanding how CEBRA can reveal shared neural dynamics across your multi-session EEG recordings, enabling insights into the common patterns of brain activity that underlie cognitive and behavioral processes.</p>"},{"location":"theory/neural-state-space/","title":"Neural State Space Analysis","text":""},{"location":"theory/neural-state-space/#conceptual-framework","title":"Conceptual Framework","text":"<p>Neural state space analysis is a mathematical framework for understanding brain activity as trajectories through a high-dimensional space, where each dimension represents the activity of a neural population or brain region. In the context of EEG analysis, this approach treats the electrical activity recorded from different electrodes as coordinates in a multi-dimensional space. EEG recordings capture electrical activity from multiple brain regions simultaneously, giving us a window into these dynamic brain states</p>"},{"location":"theory/neural-state-space/#state-space","title":"State Space","text":"<p>A neural state at time is defined as the pattern of activity across all recorded neural units. The different activity patterns of the brain can respond to:  - Attention states - Decision-making states - Motor preparation states - Cognitive load states</p>"},{"location":"theory/neural-state-space/#neural-dynamics-as-trajectories","title":"Neural Dynamics as Trajectories","text":"<p>Brain activity over time can be viewed as a trajectory  through the state space. This trajectory captures: - Instantaneous states: Brain activity at specific moments - Transitions: How the brain moves between different states - Temporal structure: The ordered sequence of states over time</p>"},{"location":"theory/neural-state-space/#discrete-vs-continuous-states","title":"Discrete vs. Continuous States","text":"<p>Neural states can be conceptualized as:</p> <ol> <li>Discrete states: Distinct, stable patterns of activity</li> <li>Decision states, attention states, motor preparation states</li> <li>Characterized by clustering in state space</li> <li> <p>Transitions between states are relatively rapid</p> </li> <li> <p>Continuous states: Smoothly varying patterns of activity</p> </li> <li>Gradual changes in attention or arousal</li> <li>Characterized by smooth trajectories in state space</li> <li>Transitions are gradual and continuous</li> </ol>"},{"location":"theory/neural-state-space/#dimensionality-and-manifold-structure","title":"Dimensionality and Manifold Structure","text":""},{"location":"theory/neural-state-space/#neural-manifolds","title":"Neural Manifolds","text":"<p>Despite the high dimensionalityof data, neural activity often lies on lower-dimensional manifolds:</p> <p>This manifold structure arises because: - Correlated activity: Neural populations often act together - Functional constraints: Brain activity serves specific computational purposes - Anatomical connectivity: Physical connections limit possible activity patterns</p>"},{"location":"theory/neural-state-space/#shared-manifold","title":"Shared Manifold","text":"<p>The shared manifold hypothesis suggests that despite individual differences, there exists a common low-dimensional space that captures: -Universal neural computations: Common brain processes across individuals -Task-related dynamics: Shared patterns of state transitions -Cognitive states: Common modes of brain activity</p> <p>This theoretical framework provides the foundation for understanding how neural state space analysis can reveal the shared patterns of brain activity across your multi-session EEG recordings, enabling insights into the common cognitive and neural processes that underlie human behavior.</p>"}]}